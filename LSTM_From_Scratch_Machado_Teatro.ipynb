{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amarabuco/deeplearning-2024.2/blob/main/LSTM_From_Scratch_Machado_Teatro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'shakespeare-text:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F759820%2F1311864%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240930%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240930T230014Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D8641ef843214982ce77ac6acd44d4ed8456bee24f92d48dee070c4b06f85950901207485ac4bab5be85b9a7cf5190adcfec831bf5bb63c0b6ffb392fe5b56290e5f8209bb858cb4145b236d0d5d26f7f9a26087199f983ad3d4c5a9c00f094283f4ac6445a9e636e08be7e0016b84888438df4ca19b8a96b273e13639d3aef6aca814505c41b966cd6c49809935b24df0c987fdf0ccff23e9cea210e16f5af7afcbf0c575cc4156e02559498946ec12f8821a9468f330a7f3a563ad9224bf7ef13c0f67c47323e23e0713de5d3c021945a86fa03f0ecb64ab6e18154ac389cb184f5200e06f4ebd8cfec301d062696533fa0130d450d476310d4a8e39d383507'\n",
        "\n",
        "\n",
        "DATA_SOURCE_MAPPING = 'machado-de-assis:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F134301%2F319174%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20241003%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20241003T113414Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D16ed6c2f34e181423afa067781bc9e2ea47f6bd85466db594b50313e4ee67198153754d2b9a63979b57b636dd124a8e45e08f6f0369f17b1102fd048aa1a605a418e3f6b62b968fb0c470ed2ca46e428ff4269bdba31d093d778534ce894eeaf8f189e0d1960954473263f5f41f04bfd54b57794b829fe6b522eab1437383b130471c4832084ebb06d343d3d9be150faa153f329032b2954afaacdddf054115a6c21b3fb80f2a2a1268d903303956e0d20c47e47b10cfb5d573a7f05c168a973d57a77ff60a607c56b8c3d059ca2eb3660f9f549e76c530c9674566c7c79d1c79d6e90dafb836f63e71227b533049c5e89682d46a4a9d5e7530ca2b625a32c1a'\n",
        "\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "ZviQ9gDHKbzJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67db701d-6980-4c74-fa96-7e83f429a49d"
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading machado-de-assis, 26705568 bytes compressed\n",
            "[==================================================] 26705568 bytes downloaded\n",
            "Downloaded and uncompressed: machado-de-assis\n",
            "Data source import complete.\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "open('/kaggle/input/machado-de-assis/raw/txt/teatro/naoConsultesMedico.txt', 'r').read().split('\\n')[:10] # Dataset é um conjunto de nomes de dinossauros\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CCa0dRl7wGwr",
        "outputId": "3211b167-125a-45b7-834c-e65dceae6ca2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Não consultes médico',\n",
              " '',\n",
              " 'PERSONAGENS',\n",
              " 'D. Leocádia',\n",
              " 'D. Carlota',\n",
              " 'D. Adelaide',\n",
              " 'Cavalcante',\n",
              " 'Magalhães',\n",
              " 'Um gabinete em casa de Magalhães, na Tijuca',\n",
              " 'CENA I']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"padding:10px;\n",
        "            color:#FF9F00;\n",
        "            margin:10px;\n",
        "            font-size:150%;\n",
        "            display:fill;\n",
        "            border-radius:1px;\n",
        "            border-style: solid;\n",
        "            border-color:#FF9F00;\n",
        "            background-color:#3E3D53;\n",
        "            overflow:hidden;\">\n",
        "    <center>\n",
        "        <a id='top'></a>\n",
        "        <b>Table of Contents</b>\n",
        "    </center>\n",
        "    <br>\n",
        "    <ul>\n",
        "        <li>\n",
        "            <a href=\"#1\">1 -  Overview and Imports</a>\n",
        "        </li>\n",
        "        <li>\n",
        "            <a href=\"#2\">2 - Data Preparation</a>\n",
        "        </li>\n",
        "        <li>\n",
        "            <a href=\"#3\">3 - LSTM Implementation</a>\n",
        "        <li>\n",
        "            <a href=\"#4\">4 - Thank you</a>\n",
        "        </li>\n",
        "        <li>\n",
        "            <a href=\"#5\">5 - References</a>\n",
        "        </li>\n",
        "    </ul>\n",
        "</div>\n",
        "<a id=\"1\"></a>\n",
        "\n",
        "<h1 style='background:#FF9F00;border:0; color:black;\n",
        "    box-shadow: 10px 10px 5px 0px rgba(0,0,0,0.75);\n",
        "    transform: rotateX(10deg);\n",
        "    '><center style='color: #3E3D53;'>Overview and Imports</center></h1>\n",
        "    \n",
        "# Overview and Imports\n",
        "\n",
        "**Long Short-Term Memory (LSTM) networks are a type of Recurrent Neural Network (RNN) that are designed to handle sequential data, such as time series or natural language text. They achieve this by using a memory cell that can selectively remember or forget information at each time step of the input sequence, allowing the network to maintain a memory of previous inputs over a longer period of time.**\n",
        "\n",
        "**This notebook contains an implementation of an LSTM that can be used for language modeling. The self takes in a sequence of characters and outputs the probability distribution over the next character in the sequence. The network is trained on a corpus of text and then used to generate new text that has a similar distribution of characters as the training corpus.**"
      ],
      "metadata": {
        "id": "GjbzNvpJKbzL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "import cupy as np # substituir numpy por cupy para por usar GPU"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-08T14:33:00.53405Z",
          "iopub.execute_input": "2023-03-08T14:33:00.534438Z",
          "iopub.status.idle": "2023-03-08T14:33:00.54035Z",
          "shell.execute_reply.started": "2023-03-08T14:33:00.534404Z",
          "shell.execute_reply": "2023-03-08T14:33:00.538848Z"
        },
        "trusted": true,
        "id": "19SRp-A2KbzO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " <a id=\"2\"></a>\n",
        "<h1 style='background:#FF9F00;border:0; color:black;\n",
        "    box-shadow: 10px 10px 5px 0px rgba(0,0,0,0.75);\n",
        "    transform: rotateX(10deg);\n",
        "    '><center style='color: #3E3D53;'>Data Preparation</center></h1>\n",
        "    \n",
        "# Data Prepartion"
      ],
      "metadata": {
        "id": "5ERwoyxAKbzO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DataGenerator:\n",
        "    \"\"\"\n",
        "    A class for reading and preprocessing text data.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, path: str, sequence_length: int):\n",
        "        \"\"\"\n",
        "        Initializes a DataReader object with the path to a text file and the desired sequence length.\n",
        "\n",
        "        Args:\n",
        "            path (str): The path to the text file.\n",
        "            sequence_length (int): The length of the sequences that will be fed to the self.\n",
        "        \"\"\"\n",
        "        with open(path) as f:\n",
        "            # Read the contents of the file\n",
        "            self.data = f.read()\n",
        "\n",
        "        # Find all unique characters in the text\n",
        "        chars = list(set(self.data))\n",
        "\n",
        "        # Create dictionaries to map characters to indices and vice versa\n",
        "        self.char_to_idx = {ch: i for (i, ch) in enumerate(chars)}\n",
        "        self.idx_to_char = {i: ch for (i, ch) in enumerate(chars)}\n",
        "\n",
        "        # Store the size of the text data and the size of the vocabulary\n",
        "        self.data_size = len(self.data)\n",
        "        self.vocab_size = len(chars)\n",
        "\n",
        "        # Initialize the pointer that will be used to generate sequences\n",
        "        self.pointer = 0\n",
        "\n",
        "        # Store the desired sequence length\n",
        "        self.sequence_length = sequence_length\n",
        "\n",
        "\n",
        "    def next_batch(self):\n",
        "        \"\"\"\n",
        "        Generates a batch of input and target sequences.\n",
        "\n",
        "        Returns:\n",
        "            inputs_one_hot (np.ndarray): A numpy array with shape `(batch_size, vocab_size)` where each row is a one-hot encoded representation of a character in the input sequence.\n",
        "            targets (list): A list of integers that correspond to the indices of the characters in the target sequence, which is the same as the input sequence shifted by one position to the right.\n",
        "        \"\"\"\n",
        "        input_start = self.pointer\n",
        "        input_end = self.pointer + self.sequence_length\n",
        "\n",
        "        # Get the input sequence as a list of integers\n",
        "        inputs = [self.char_to_idx[ch] for ch in self.data[input_start:input_end]]\n",
        "\n",
        "        # One-hot encode the input sequence\n",
        "        inputs_one_hot = np.zeros((len(inputs), self.vocab_size))\n",
        "        inputs_one_hot[np.arange(len(inputs)), inputs] = 1\n",
        "\n",
        "        # Get the target sequence as a list of integers\n",
        "        targets = [self.char_to_idx[ch] for ch in self.data[input_start + 1:input_end + 1]]\n",
        "\n",
        "        # Update the pointer\n",
        "        self.pointer += self.sequence_length\n",
        "\n",
        "        # Reset the pointer if the next batch would exceed the length of the text data\n",
        "        if self.pointer + self.sequence_length + 1 >= self.data_size:\n",
        "            self.pointer = 0\n",
        "\n",
        "        return inputs_one_hot, targets"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-08T14:33:00.956776Z",
          "iopub.execute_input": "2023-03-08T14:33:00.958519Z",
          "iopub.status.idle": "2023-03-08T14:33:00.971932Z",
          "shell.execute_reply.started": "2023-03-08T14:33:00.958428Z",
          "shell.execute_reply": "2023-03-08T14:33:00.970673Z"
        },
        "trusted": true,
        "id": "NppOie_uKbzP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"3\"></a>\n",
        "<h1 style='background:#FF9F00;border:0; color:black;\n",
        "    box-shadow: 10px 10px 5px 0px rgba(0,0,0,0.75);\n",
        "    transform: rotateX(10deg);\n",
        "    '><center style='color: #3E3D53;'>LSTM Implementation</center></h1>\n",
        "\n",
        "\n",
        "# LSTM Implementation\n"
      ],
      "metadata": {
        "id": "rekyNvRqKbzQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Forward Propagation\n",
        "\n",
        "**The forward pass is a step in training a simple LSTM (Long Short-Term Memory) model. During the forward pass, the model takes an input sequence X and performs a series of computations to generate a sequence of output probability vectors y_pred, which represent the predicted probability distribution over the possible output classes for each time step in the input sequence.**\n",
        "\n",
        "**The LSTM model has a memory cell that stores information from previous time steps, which is updated based on the input sequence and a set of learned parameters. During the forward pass, the LSTM computes a set of vectors representing the forget gate, input gate, candidate cell state, and output gate for each time step in the input sequence.**\n",
        "\n",
        "**These vectors are used to update the cell state and hidden state for each time step, which are then used to generate the output probability vectors. The function returns the input sequence, the cell state for each time step, the forget gate, input gate, output gate, candidate cell state, hidden state, and output probability vector for each time step.**\n",
        "\n",
        "**Here are the formulas used in the code:**\n",
        "\n",
        "**Concatenation of the input and hidden state:**\n",
        "$$\\text{concat} = \\begin{bmatrix} a_{t-1} \\ x_t \\end{bmatrix}$$\n",
        "\n",
        "**Forget gate:**\n",
        "**$$f_t = \\sigma(W_f \\text{concat} + b_f)$$**\n",
        "\n",
        "**Input gate:**\n",
        "**$$i_t = \\sigma(W_i \\text{concat} + b_i)$$**\n",
        "\n",
        "**Candidate cell state:**\n",
        "**$$\\tilde{c}_t = \\tanh(W_c \\text{concat} + b_c)$$**\n",
        "\n",
        "**Cell state:**\n",
        "**$$c_t = f_t \\odot c_{t-1} + i_t \\odot \\tilde{c}_t$$**\n",
        "\n",
        "**Output gate:**\n",
        "**$$o_t = \\sigma(W_o \\text{concat} + b_o)$$**\n",
        "\n",
        "**Hidden state:**\n",
        "**$$a_t = o_t \\odot \\tanh(c_t)$$**\n",
        "\n",
        "**Output probability vector:**\n",
        "**$$\\hat{y}_t = \\text{softmax}(W_y a_t + b_y)$$**\n",
        "\n",
        "**Where:**\n",
        "\n",
        "**$x_t$ is the input vector at time $t$.**\n",
        "\n",
        "**$a_{t-1}$ is the hidden state vector at time $t-1$.**\n",
        "\n",
        "**$f_t$, $i_t$, $o_t$ are the forget, input, and output gates at time $t$, respectively.**\n",
        "\n",
        "**$c_t$ is the cell state vector at time $t$.**\n",
        "\n",
        "**$\\tilde{c}_t$ is the candidate cell state at time $t$.**\n",
        "\n",
        "$\\hat{y}_t$ is the output probability vector at time $t$.\n",
        "\n",
        "**$\\odot$ is the element-wise multiplication operator.**\n",
        "\n",
        "**$\\sigma$ is the sigmoid activation function.**\n",
        "\n",
        "**$\\text{tanh}$ is the hyperbolic tangent activation function.**\n",
        "\n",
        "**$W_f, W_i, W_c, W_o, W_y$ are the weight matrices for the forget, input, candidate, output, and output layers, respectively.**\n",
        "\n",
        "**$b_f, b_i, b_c, b_o, b_y$ are the bias vectors for the forget, input, candidate, output, and output layers, respectively.**\n",
        "****\n",
        "## Backpropagation\n",
        "**The backpropagation step in training a simple LSTM model involves computing the gradients of the loss with respect to the model parameters, which are used to update the parameters in the opposite direction of the gradient to minimize the loss function.**\n",
        "\n",
        "**In this particular implementation of the LSTM model, the backpropagation step is implemented using the standard backpropagation through time (BPTT) algorithm. The algorithm iteratively computes the gradients of the loss with respect to each parameter in the model by propagating the gradients backwards through time from the output sequence to the input sequence.**\n",
        "\n",
        "**Starting from the final time step, the gradients of the loss with respect to the output probability vectors are computed using the cross-entropy loss function. The gradients are then propagated backwards through time by computing the gradients of the loss with respect to the hidden state and cell state for each time step.**\n",
        "\n",
        "**These gradients are used to compute the gradients of the loss with respect to the output gate, candidate cell state, input gate, and forget gate vectors for each time step, which are in turn used to compute the gradients of the loss with respect to the weight matrices and bias vectors for each gate and the output layer.**\n",
        "\n",
        "**The gradients are then accumulated across all time steps and used to update the model parameters using an optimization algorithm, such as stochastic gradient descent (SGD). This process is repeated for each batch of input sequences during training, until the model converges to a set of optimal parameters that minimize the loss function.**\n",
        "\n",
        "$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W_y}} = \\sum_{t=1}^T \\mathbf{a}_t \\cdot (\\mathbf{y}_t - \\mathbf{t}_t)^\\top$$\n",
        "\n",
        "$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{b_y}} = \\sum_{t=1}^T (\\mathbf{y}_t - \\mathbf{t}_t)$$\n",
        "\n",
        "$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{a}_t} = \\mathbf{W_y}^\\top \\cdot (\\mathbf{y}_t - \\mathbf{t}t) + \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{a}{t+1}} \\cdot \\mathbf{W_f}^\\top \\cdot \\mathbf{f}_t + \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{c}_t} \\cdot \\mathbf{W_c}^\\top \\cdot \\mathbf{i}_t + \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{o}_t} \\cdot \\mathbf{W_o}^\\top \\cdot \\mathbf{o}_t$$\n",
        "\n",
        "$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{c}t} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{c}{t+1}} \\cdot \\mathbf{f}_t + \\mathbf{i}_t \\cdot \\mathbf{c}_t \\cdot (1 - \\mathbf{c}_t) \\cdot \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{a}_t}$$\n",
        "\n",
        "$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{i}_t} = \\mathbf{c}_t \\cdot (1 - \\mathbf{i}_t) \\cdot \\mathbf{i}_t \\cdot \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{c}_t}$$\n",
        "\n",
        "$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{f}t} = \\mathbf{c}{t-1} \\cdot (1 - \\mathbf{f}_t) \\cdot \\mathbf{f}_t \\cdot \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{c}_t}$$\n",
        "\n",
        "$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{o}_t} = \\mathbf{a}_t \\cdot (1 - \\mathbf{o}_t) \\cdot \\mathbf{o}_t \\cdot \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{c}_t}$$\n",
        "\n",
        "$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{c}_{t-1}} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{c}_t} \\cdot \\mathbf{f}_t$$\n",
        "\n",
        "$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W_c}} = \\sum_{t=1}^T \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{c}_t} \\cdot \\mathbf{i}_t \\cdot (1 - \\mathbf{c}_t^2) \\cdot \\mathbf{concat}_t^\\top$$\n",
        "****\n",
        "## Loss:\n",
        "\n",
        "**The cross-entropy loss between the predicted probabilities y_pred and the true targets y_true at a single time step $t$ is:**\n",
        "\n",
        "**$$H(y_{true,t}, y_{pred,t}) = -\\sum_i y_{true,t,i} \\log(y_{pred,t,i})$$**\n",
        "\n",
        "**where $y_{pred,t}$ is the predicted probability distribution at time step $t$, $y_{true,t}$ is the true probability distribution at time step $t$ (i.e., a one-hot encoded vector representing the true target), and $i$ ranges over the vocabulary size.**\n",
        "\n",
        "**The total loss is then computed as the sum of the cross-entropy losses over all time steps:**\n",
        "\n",
        "**$$L = \\sum_{t=1}^{T} H(y_{true,t}, y_{pred,t})$$**\n",
        "\n",
        "**where $T$ is the sequence length.**\n",
        "\n",
        "****\n",
        "## Optimization\n",
        "\n",
        "\n",
        "**The AdamW optimizer is an extension of the Adam optimizer that incorporates weight decay directly into the update rule, rather than treating it as a separate regularization term. This has been shown to improve the generalization performance of deep neural networks.**\n",
        "\n",
        "**To use the AdamW optimizer in practice, we first initialize the parameters $W_f$, $b_f$, $W_i$, $b_i$, $W_c$, $b_c$, $W_o$, and $b_o$ with small random values. Then, for each training iteration, we compute the gradients of the loss function with respect to the parameters using backpropagation. These gradients are then used to update the parameters using the AdamW update rules described above. This process is repeated for a fixed number of iterations or until convergence is achieved.**\n",
        "\n",
        "**In summary, the AdamW optimizer is a powerful optimization algorithm for training deep neural networks. By incorporating weight decay directly into the update rule, it can improve the generalization performance of the network and reduce the risk of overfitting.**\n",
        "\n",
        "**For the AdamW update of parameter Wf:**\n",
        "\n",
        "$m_{Wf} \\leftarrow \\beta_1 m_{Wf} + (1 - \\beta_1) \\nabla_{Wf} J$\n",
        "\n",
        "$v_{Wf} \\leftarrow \\beta_2 v_{Wf} + (1 - \\beta_2) (\\nabla_{Wf} J)^2$\n",
        "\n",
        "$\\hat{m}{Wf} = \\frac{m{Wf}}{1 - \\beta_1}$\n",
        "\n",
        "$\\hat{v}{Wf} = \\frac{v{Wf}}{1 - \\beta_2}$\n",
        "\n",
        "$W_f \\leftarrow W_f - \\eta \\frac{\\hat{m}{Wf}}{\\sqrt{\\hat{v}{Wf}} + \\epsilon} - \\eta \\lambda_2 W_f$\n",
        "\n",
        "**For the AdamW update of parameter bf:**\n",
        "\n",
        "$m_{bf} \\leftarrow \\beta_1 m_{bf} + (1 - \\beta_1) \\nabla_{bf} J$\n",
        "\n",
        "$v_{bf} \\leftarrow \\beta_2 v_{bf} + (1 - \\beta_2) (\\nabla_{bf} J)^2$\n",
        "\n",
        "$\\hat{m}{bf} = \\frac{m{bf}}{1 - \\beta_1}$\n",
        "\n",
        "$\\hat{v}{bf} = \\frac{v{bf}}{1 - \\beta_2}$\n",
        "\n",
        "$b_f \\leftarrow b_f - \\eta \\frac{\\hat{m}{bf}}{\\sqrt{\\hat{v}{bf}} + \\epsilon} - \\eta \\lambda_2 b_f$\n",
        "\n",
        "**For the AdamW update of parameter Wi:**\n",
        "\n",
        "$m_{Wi} \\leftarrow \\beta_1 m_{Wi} + (1 - \\beta_1) \\nabla_{Wi} J$\n",
        "\n",
        "$v_{Wi} \\leftarrow \\beta_2 v_{Wi} + (1 - \\beta_2) (\\nabla_{Wi} J)^2$\n",
        "\n",
        "$\\hat{m}{Wi} = \\frac{m{Wi}}{1 - \\beta_1}$\n",
        "\n",
        "$\\hat{v}{Wi} = \\frac{v{Wi}}{1 - \\beta_2}$\n",
        "\n",
        "$W_i \\leftarrow W_i - \\eta \\frac{\\hat{m}{Wi}}{\\sqrt{\\hat{v}{Wi}} + \\epsilon} - \\eta \\lambda_2 W_i$\n",
        "\n",
        "**For the AdamW update of parameter bi:**\n",
        "\n",
        "$m_{bi} \\leftarrow \\beta_1 m_{bi} + (1 - \\beta_1) \\nabla_{bi} J$\n",
        "\n",
        "$v_{bi} \\leftarrow \\beta_2 v_{bi} + (1 - \\beta_2) (\\nabla_{bi} J)^2$\n",
        "\n",
        "$\\hat{m}{bi} = \\frac{m{bi}}{1 - \\beta_1}$\n",
        "\n",
        "$\\hat{v}{bi} = \\frac{v{bi}}{1 - \\beta_2}$\n",
        "\n",
        "$b_i \\leftarrow b_i - \\eta \\frac{\\hat{m}{bi}}{\\sqrt{\\hat{v}{bi}} + \\epsilon} - \\eta \\lambda_2 b_i$\n",
        "\n",
        "**For the AdamW update of parameter Wc:**\n",
        "\n",
        "$m_{Wc} \\leftarrow \\beta_1 m_{Wc} + (1 - \\beta_1) \\nabla_{Wc} J$\n",
        "\n",
        "$v_{Wc} \\leftarrow \\beta_2 v_{Wc} + (1 - \\beta2) (\\nabla{Wc} J)^2$\n",
        "\n",
        "$\\hat{m}{Wc} = \\frac{m{Wc}}{1 - \\beta_1}$\n",
        "\n",
        "$\\hat{v}{Wc} = \\frac{v{Wc}}{1 - \\beta_2}$\n",
        "\n",
        "$W_c \\leftarrow W_c - \\eta \\frac{\\hat{m}{Wc}}{\\sqrt{\\hat{v}{Wc}} + \\epsilon} - \\eta \\lambda_2 W_c$\n",
        "\n",
        "**For the AdamW update of parameter bc:**\n",
        "\n",
        "$m_{bc} \\leftarrow \\beta_1 m_{bc} + (1 - \\beta_1) \\nabla_{bc} J$\n",
        "\n",
        "$v_{bc} \\leftarrow \\beta_2 v_{bc} + (1 - \\beta_2) (\\nabla_{bc} J)^2$\n",
        "\n",
        "$\\hat{m}{bc} = \\frac{m{bc}}{1 - \\beta_1}$\n",
        "\n",
        "$\\hat{v}{bc} = \\frac{v{bc}}{1 - \\beta_2}$\n",
        "\n",
        "$b_c \\leftarrow b_c - \\eta \\frac{\\hat{m}{bc}}{\\sqrt{\\hat{v}{bc}} + \\epsilon} - \\eta \\lambda_2 b_c$\n",
        "\n",
        "**For the AdamW update of parameter Wo:**\n",
        "\n",
        "$m_{Wo} \\leftarrow \\beta_1 m_{Wo} + (1 - \\beta_1) \\nabla_{Wo} J$\n",
        "\n",
        "$v_{Wo} \\leftarrow \\beta_2 v_{Wo} + (1 - \\beta_2) (\\nabla_{Wo} J)^2$\n",
        "\n",
        "$\\hat{m}{Wo} = \\frac{m{Wo}}{1 - \\beta_1}$\n",
        "\n",
        "$\\hat{v}{Wo} = \\frac{v{Wo}}{1 - \\beta_2}$\n",
        "\n",
        "$W_o \\leftarrow W_o - \\eta \\frac{\\hat{m}{Wo}}{\\sqrt{\\hat{v}{Wo}} + \\epsilon} - \\eta \\lambda_2 W_o$\n",
        "\n",
        "**For the AdamW update of parameter bo:**\n",
        "\n",
        "$m_{bo} \\leftarrow \\beta_1 m_{bo} + (1 - \\beta_1) \\nabla_{bo} J$\n",
        "\n",
        "$v_{bo} \\leftarrow \\beta_2 v_{bo} + (1 - \\beta_2) (\\nabla_{bo} J)^2$\n",
        "\n",
        "$\\hat{m}{bo} = \\frac{m{bo}}{1 - \\beta_1}$\n",
        "\n",
        "$\\hat{v}{bo} = \\frac{v{bo}}{1 - \\beta_2}$\n",
        "\n",
        "**$b_o \\leftarrow b_o - \\eta \\frac{\\hat{m}{bo}}{\\sqrt{\\hat{v}{bo}} + \\epsilon} - \\eta \\lambda_2 b_o$**\n",
        "\n",
        "**In the above formulas, $\\nabla_{Wf} J$, $\\nabla_{bf} J$, $\\nabla_{Wi} J$, $\\nabla_{bi} J$, $\\nabla_{Wc} J$, $\\nabla_{bc} J$, $\\nabla_{Wo} J$, and $\\nabla_{bo} J$ are the gradients of the loss function J with respect to the parameters $W_f$, $b_f$, $W_i$, $b_i$, $W_c$, $b_c$, $W_o$, and $b_o$ respectively. $\\beta_1$ and $\\beta_2$ are hyperparameters controlling the exponential decay rates of the moving averages. $\\eta$ is the learning rate, $\\lambda_2$ is the weight decay hyperparameter, and $\\epsilon$ is a small constant to avoid division by zero.**\n",
        "\n",
        "\n",
        "\n",
        "****\n",
        "## Train\n",
        "\n",
        "**The train method trains the LSTM on a dataset using backpropagation through time. The method takes an instance of DataReader containing the training data as input. The method initializes a hidden state vector a_prev at the beginning of each sequence to zero.**\n",
        "\n",
        "**It then iterates until the smooth loss is less than a threshold value. During each iteration, it retrieves a batch of inputs and targets from the data generator.**\n",
        "\n",
        "**The LSTM then performs a forward pass on the input sequence and computes the output probabilities. The backward pass is performed using the targets and output probabilities to calculate the gradients of the parameters of the network.**\n",
        "\n",
        "**The AdamW algorithm is used to update the weights of the network. The method then calculates and updates the loss using the updated weights. The previous hidden state is updated for the next batch.**\n",
        "\n",
        "**The method prints the progress every 10000 iterations by generating a sample of text using the sample method and printing the loss.**\n",
        "\n",
        "**The train method can be summarized by the following steps:**\n",
        "\n",
        "**$1.$ Initialize $a_{prev}$ to zero at the beginning of each sequence.**\n",
        "\n",
        "**$2.$ Retrieve a batch of inputs and targets from the data generator.**\n",
        "\n",
        "**$3.$ Perform a forward pass on the input sequence and compute the output probabilities.**\n",
        "\n",
        "**$4.$ Perform a backward pass using the targets and output probabilities to calculate the gradients of the parameters of the network.**\n",
        "\n",
        "**$5.$ Use the AdamW algorithm to update the weights of the network.**\n",
        "\n",
        "**$6.$ Calculate and update the loss using the updated weights.**\n",
        "\n",
        "**$7.$ Update the previous hidden state for the next batch.**\n",
        "\n",
        "**$8.$ Print progress every 1000 iterations by generating a sample of text using the sample method and printing the loss.**\n",
        "\n",
        "**$9.$ Repeat steps $2$-$8$ until the smooth loss is less than the threshold value.**"
      ],
      "metadata": {
        "id": "-MXk4NJNKbzQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " class LSTM:\n",
        "    \"\"\"\n",
        "    A class used to represent a Recurrent Neural Network (LSTM).\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    hidden_size : int\n",
        "        The number of hidden units in the LSTM.\n",
        "    vocab_size : int\n",
        "        The size of the vocabulary used by the LSTM.\n",
        "    sequence_length : int\n",
        "        The length of the input sequences fed to the LSTM.\n",
        "    self.learning_rate : float\n",
        "        The learning rate used during training.\n",
        "\n",
        "    Methods\n",
        "    -------\n",
        "    __init__(hidden_size, vocab_size, sequence_length, self.learning_rate)\n",
        "        Initializes an instance of the LSTM class.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, hidden_size, vocab_size, sequence_length, learning_rate):\n",
        "        \"\"\"\n",
        "        Initializes an instance of the LSTM class.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        hidden_size : int\n",
        "            The number of hidden units in the LSTM.\n",
        "        vocab_size : int\n",
        "            The size of the vocabulary used by the LSTM.\n",
        "        sequence_length : int\n",
        "            The length of the input sequences fed to the LSTM.\n",
        "        learning_rate : float\n",
        "            The learning rate used during training.\n",
        "        \"\"\"\n",
        "        # hyper parameters\n",
        "        self.mby = None\n",
        "        self.hidden_size = hidden_size\n",
        "        self.vocab_size = vocab_size\n",
        "        self.sequence_length = sequence_length\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        # model parameters\n",
        "        self.Wf = np.random.uniform(-np.sqrt(1. / hidden_size), np.sqrt(1. / hidden_size),\n",
        "                                    (hidden_size, hidden_size + vocab_size))\n",
        "        self.bf = np.zeros((hidden_size, 1))\n",
        "\n",
        "        self.Wi = np.random.uniform(-np.sqrt(1. / hidden_size), np.sqrt(1. / hidden_size),\n",
        "                                    (hidden_size, hidden_size + vocab_size))\n",
        "        self.bi = np.zeros((hidden_size, 1))\n",
        "\n",
        "        self.Wc = np.random.uniform(-np.sqrt(1. / hidden_size), np.sqrt(1. / hidden_size),\n",
        "                                    (hidden_size, hidden_size + vocab_size))\n",
        "        self.bc = np.zeros((hidden_size, 1))\n",
        "\n",
        "        self.Wo = np.random.uniform(-np.sqrt(1. / hidden_size), np.sqrt(1. / hidden_size),\n",
        "                                    (hidden_size, hidden_size + vocab_size))\n",
        "        self.bo = np.zeros((hidden_size, 1))\n",
        "\n",
        "        self.Wy = np.random.uniform(-np.sqrt(1. / hidden_size), np.sqrt(1. / hidden_size),\n",
        "                                    (vocab_size, hidden_size))\n",
        "        self.by = np.zeros((vocab_size, 1))\n",
        "\n",
        "        # initialize parameters for adamw optimizer\n",
        "        self.mWf = np.zeros_like(self.Wf)\n",
        "        self.vWf = np.zeros_like(self.Wf)\n",
        "        self.mWi = np.zeros_like(self.Wi)\n",
        "        self.vWi = np.zeros_like(self.Wi)\n",
        "        self.mWc = np.zeros_like(self.Wc)\n",
        "        self.vWc = np.zeros_like(self.Wc)\n",
        "        self.mWo = np.zeros_like(self.Wo)\n",
        "        self.vWo = np.zeros_like(self.Wo)\n",
        "        self.mWy = np.zeros_like(self.Wy)\n",
        "        self.vWy = np.zeros_like(self.Wy)\n",
        "        self.mbf = np.zeros_like(self.bf)\n",
        "        self.vbf = np.zeros_like(self.bf)\n",
        "        self.mbi = np.zeros_like(self.bi)\n",
        "        self.vbi = np.zeros_like(self.bi)\n",
        "        self.mbc = np.zeros_like(self.bc)\n",
        "        self.vbc = np.zeros_like(self.bc)\n",
        "        self.mbo = np.zeros_like(self.bo)\n",
        "        self.vbo = np.zeros_like(self.bo)\n",
        "        self.mby = np.zeros_like(self.by)\n",
        "        self.vby = np.zeros_like(self.by)\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        \"\"\"\n",
        "        Computes the sigmoid activation function for a given input array.\n",
        "\n",
        "        Parameters:\n",
        "            x (ndarray): Input array.\n",
        "\n",
        "        Returns:\n",
        "            ndarray: Array of the same shape as `x`, containing the sigmoid activation values.\n",
        "        \"\"\"\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def softmax(self, x):\n",
        "        \"\"\"\n",
        "        Computes the softmax activation function for a given input array.\n",
        "\n",
        "        Parameters:\n",
        "            x (ndarray): Input array.\n",
        "\n",
        "        Returns:\n",
        "            ndarray: Array of the same shape as `x`, containing the softmax activation values.\n",
        "        \"\"\"\n",
        "        # shift the input to prevent overflow when computing the exponentials\n",
        "        x = x - np.max(x)\n",
        "        # compute the exponentials of the shifted input\n",
        "        p = np.exp(x)\n",
        "        # normalize the exponentials by dividing by their sum\n",
        "        return p / np.sum(p)\n",
        "\n",
        "    def loss(self, y_preds, targets):\n",
        "        \"\"\"\n",
        "        Computes the cross-entropy loss for a given sequence of predicted probabilities and true targets.\n",
        "\n",
        "        Parameters:\n",
        "            y_preds (ndarray): Array of shape (sequence_length, vocab_size) containing the predicted probabilities for each time step.\n",
        "            targets (ndarray): Array of shape (sequence_length, 1) containing the true targets for each time step.\n",
        "\n",
        "        Returns:\n",
        "            float: Cross-entropy loss.\n",
        "        \"\"\"\n",
        "        # calculate cross-entropy loss\n",
        "        return sum(-np.log(y_preds[t][targets[t], 0]) for t in range(self.sequence_length))\n",
        "\n",
        "\n",
        "    def adamw(self, beta1=0.9, beta2=0.999, epsilon=1e-8, L2_reg=1e-4):\n",
        "        \"\"\"\n",
        "        Updates the LSTM's parameters using the AdamW optimization algorithm.\n",
        "        \"\"\"\n",
        "        # AdamW update for Wf\n",
        "        self.mWf = beta1 * self.mWf + (1 - beta1) * self.dWf\n",
        "        self.vWf = beta2 * self.vWf + (1 - beta2) * np.square(self.dWf)\n",
        "        m_hat = self.mWf / (1 - beta1)\n",
        "        v_hat = self.vWf / (1 - beta2)\n",
        "        self.Wf -= self.learning_rate * (m_hat / (np.sqrt(v_hat) + epsilon) + L2_reg * self.Wf)\n",
        "\n",
        "        # AdamW update for bf\n",
        "        self.mbf = beta1 * self.mbf + (1 - beta1) * self.dbf\n",
        "        self.vbf = beta2 * self.vbf + (1 - beta2) * np.square(self.dbf)\n",
        "        m_hat = self.mbf / (1 - beta1)\n",
        "        v_hat = self.vbf / (1 - beta2)\n",
        "        self.bf -= self.learning_rate * (m_hat / (np.sqrt(v_hat) + epsilon) + L2_reg * self.bf)\n",
        "\n",
        "        # AdamW update for Wi\n",
        "        self.mWi = beta1 * self.mWi + (1 - beta1) * self.dWi\n",
        "        self.vWi = beta2 * self.vWi + (1 - beta2) * np.square(self.dWi)\n",
        "        m_hat = self.mWi / (1 - beta1)\n",
        "        v_hat = self.vWi / (1 - beta2)\n",
        "        self.Wi -= self.learning_rate * (m_hat / (np.sqrt(v_hat) + epsilon) + L2_reg * self.Wi)\n",
        "\n",
        "        # AdamW update for bi\n",
        "        self.mbi = beta1 * self.mbi + (1 - beta1) * self.dbi\n",
        "        self.vbi = beta2 * self.vbi + (1 - beta2) * np.square(self.dbi)\n",
        "        m_hat = self.mbi / (1 - beta1)\n",
        "        v_hat = self.vbi / (1 - beta2)\n",
        "        self.bi -= self.learning_rate * (m_hat / (np.sqrt(v_hat) + epsilon) + L2_reg * self.bi)\n",
        "\n",
        "        # AdamW update for Wc\n",
        "        self.mWc = beta1 * self.mWc + (1 - beta1) * self.dWc\n",
        "        self.vWc = beta2 * self.vWc + (1 - beta2) * np.square(self.dWc)\n",
        "        m_hat = self.mWc / (1 - beta1)\n",
        "        v_hat = self.vWc / (1 - beta2)\n",
        "        self.Wc -= self.learning_rate * (m_hat / (np.sqrt(v_hat) + epsilon) + L2_reg * self.Wc)\n",
        "\n",
        "        # AdamW update for bc\n",
        "        self.mbc = beta1 * self.mbc + (1 - beta1) * self.dbc\n",
        "        self.vbc = beta2 * self.vbc + (1 - beta2) * np.square(self.dbc)\n",
        "        m_hat = self.mbc / (1 - beta1)\n",
        "        v_hat = self.vbc / (1 - beta2)\n",
        "        self.bc -= self.learning_rate * (m_hat / (np.sqrt(v_hat) + epsilon) + L2_reg * self.bc)\n",
        "\n",
        "        # AdamW update for Wy\n",
        "        self.mWy = beta1 * self.mWy + (1 - beta1) * self.dWy\n",
        "        self.vWy = beta2 * self.vWy + (1 - beta2) * np.square(self.dWy)\n",
        "        m_hat = self.mWy / (1 - beta1)\n",
        "        v_hat = self.vWy / (1 - beta2)\n",
        "        self.Wy -= self.learning_rate * (m_hat / (np.sqrt(v_hat) + epsilon) + L2_reg * self.Wy)\n",
        "        # AdamW update for by\n",
        "        self.mby = beta1 * self.mby + (1 - beta1) * self.dby\n",
        "        self.vby = beta2 * self.vby + (1 - beta2) * np.square(self.dby)\n",
        "        m_hat = self.mby / (1 - beta1)\n",
        "        v_hat = self.vby / (1 - beta2)\n",
        "        self.by -= self.learning_rate * (m_hat / (np.sqrt(v_hat) + epsilon) + L2_reg * self.by)\n",
        "\n",
        "\n",
        "    def forward(self, X, c_prev, a_prev):\n",
        "        \"\"\"\n",
        "        Performs forward propagation for a simple LSTM model.\n",
        "\n",
        "        Args:\n",
        "            X (numpy array): Input sequence, shape (sequence_length, input_size)\n",
        "            c_prev (numpy array): Previous cell state, shape (hidden_size, 1)\n",
        "            a_prev (numpy array): Previous hidden state, shape (hidden_size, 1)\n",
        "\n",
        "        Returns:\n",
        "            X (numpy array): Input sequence, shape (sequence_length, input_size)\n",
        "            c (dictionary): Cell state for each time step, keys = time step, values = numpy array shape (hidden_size, 1)\n",
        "            f (dictionary): Forget gate for each time step, keys = time step, values = numpy array shape (hidden_size, 1)\n",
        "            i (dictionary): Input gate for each time step, keys = time step, values = numpy array shape (hidden_size, 1)\n",
        "            o (dictionary): Output gate for each time step, keys = time step, values = numpy array shape (hidden_size, 1)\n",
        "            cc (dictionary): Candidate cell state for each time step, keys = time step, values = numpy array shape (hidden_size, 1)\n",
        "            a (dictionary): Hidden state for each time step, keys = time step, values = numpy array shape (hidden_size, 1)\n",
        "            y_pred (dictionary): Output probability vector for each time step, keys = time step, values = numpy array shape (output_size, 1)\n",
        "        \"\"\"\n",
        "        # initialize dictionaries for backpropagation\n",
        "        c, f, i, o, cc, a, y_pred = {}, {}, {}, {}, {}, {}, {}\n",
        "        c[-1] = np.copy(c_prev)  # store the initial cell state in the dictionary\n",
        "        a[-1] = np.copy(a_prev)  # store the initial hidden state in the dictionary\n",
        "\n",
        "        # iterate over each time step in the input sequence\n",
        "        for t in range(X.shape[0]):\n",
        "            # concatenate the input and hidden state\n",
        "            xt = X[t, :].reshape(-1, 1)\n",
        "            concat = np.vstack((a[t - 1], xt))\n",
        "\n",
        "            # compute the forget gate\n",
        "            f[t] = self.sigmoid(np.dot(self.Wf, concat) + self.bf)\n",
        "\n",
        "            # compute the input gate\n",
        "            i[t] = self.sigmoid(np.dot(self.Wi, concat) + self.bi)\n",
        "\n",
        "            # compute the candidate cell state\n",
        "            cc[t] = np.tanh(np.dot(self.Wc, concat) + self.bc)\n",
        "\n",
        "            # compute the cell state\n",
        "            c[t] = f[t] * c[t - 1] + i[t] * cc[t]\n",
        "\n",
        "            # compute the output gate\n",
        "            o[t] = self.sigmoid(np.dot(self.Wo, concat) + self.bo)\n",
        "\n",
        "            # compute the hidden state\n",
        "            a[t] = o[t] * np.tanh(c[t])\n",
        "\n",
        "            # compute the output probability vector\n",
        "            y_pred[t] = self.softmax(np.dot(self.Wy, a[t]) + self.by)\n",
        "\n",
        "        # return the output probability vectors, cell state, hidden state and gate vectors\n",
        "        return X, y_pred, c, f, i, o, cc, a\n",
        "\n",
        "\n",
        "    def backward(self, X, targets, y_pred, c_prev, a_prev, c, f, i, o, cc, a):\n",
        "        \"\"\"\n",
        "        Performs backward propagation through time for an LSTM network.\n",
        "\n",
        "        Args:\n",
        "        - X: input data for each time step, with shape (sequence_length, input_size)\n",
        "        - targets: target outputs for each time step, with shape (sequence_length, output_size)\n",
        "        - y_pred: predicted outputs for each time step, with shape (sequence_length, output_size)\n",
        "        - c_prev: previous cell state, with shape (hidden_size, 1)\n",
        "        - a_prev: previous hidden state, with shape (hidden_size, 1)\n",
        "        - c: cell state for each time step, with shape (sequence_length, hidden_size)\n",
        "        - f: forget gate output for each time step, with shape (sequence_length, hidden_size)\n",
        "        - i: input gate output for each time step, with shape (sequence_length, hidden_size)\n",
        "        - o: output gate output for each time step, with shape (sequence_length, hidden_size)\n",
        "        - cc: candidate cell state for each time step, with shape (sequence_length, hidden_size)\n",
        "        - a: hidden state output for each time step, with shape (sequence_length, hidden_size)\n",
        "        Returns:\n",
        "            None\n",
        "        \"\"\"\n",
        "\n",
        "        # initialize gradients for each parameter\n",
        "        self.dWf, self.dWi, self.dWc, self.dWo, self.dWy = np.zeros_like(self.Wf), np.zeros_like(self.Wi), np.zeros_like(self.Wc), np.zeros_like(self.Wo), np.zeros_like(self.Wy)\n",
        "        self.dbf, self.dbi, self.dbc, self.dbo, self.dby = np.zeros_like(self.bf), np.zeros_like(self.bi), np.zeros_like(self.bc), np.zeros_like(self.bo), np.zeros_like(self.by)\n",
        "        dc_next = np.zeros_like(c_prev)\n",
        "        da_next = np.zeros_like(a_prev)\n",
        "\n",
        "        # iterate backwards through time steps\n",
        "        for t in reversed(range(X.shape[0])):\n",
        "            # compute the gradient of the output probability vector\n",
        "            dy = np.copy(y_pred[t])\n",
        "            dy[targets[t]] -= 1\n",
        "\n",
        "            # compute the gradient of the output layer weights and biases\n",
        "            self.dWy += np.dot(dy, a[t].T)\n",
        "            self.dby += dy\n",
        "\n",
        "            # compute the gradient of the hidden state\n",
        "            da = np.dot(self.Wy.T, dy) + da_next\n",
        "            dc = dc_next + (1 - np.tanh(c[t])**2) * o[t] * da\n",
        "\n",
        "            # compute the gradient of the output gate\n",
        "            xt = X[t, :].reshape(-1, 1)\n",
        "            concat = np.vstack((a[t - 1], xt))\n",
        "            do = o[t] * (1 - o[t]) * np.tanh(c[t]) * da\n",
        "            self.dWo += np.dot(do, concat.T)\n",
        "            self.dbo += do\n",
        "\n",
        "            # compute the gradient of the candidate cell state\n",
        "            dcc = dc * i[t] * (1 - np.tanh(cc[t])**2)\n",
        "            self.dWc += np.dot(dcc, concat.T)\n",
        "            self.dbc += dcc\n",
        "\n",
        "            # compute the gradient of the input gate\n",
        "            di = i[t] * (1 - i[t]) * cc[t] * dc\n",
        "            self.dWi += np.dot(di, concat.T)\n",
        "            self.dbi += di\n",
        "\n",
        "            # compute the gradient of the forget gate\n",
        "            df = f[t] * (1 - f[t]) * c[t - 1] * dc\n",
        "            self.dWf += np.dot(df, concat.T)\n",
        "            self.dbf += df\n",
        "\n",
        "            # compute the gradient of the input to the current hidden state and cell state\n",
        "            da_next = np.dot(self.Wf[:, :self.hidden_size].T, df)\\\n",
        "            + np.dot(self.Wi[:, :self.hidden_size].T, di)\\\n",
        "            + np.dot(self.Wc[:, :self.hidden_size].T, dcc)\\\n",
        "            + np.dot(self.Wo[:, :self.hidden_size].T, do)\n",
        "            dc_next = dc * f[t]\n",
        "\n",
        "        # clip gradients to avoid exploding gradients\n",
        "        for grad in [self.dWf, self.dWi, self.dWc, self.dWo, self.dWy, self.dbf, self.dbi, self.dbc, self.dbo, self.dby]:\n",
        "            np.clip(grad, -1, 1, out=grad)\n",
        "\n",
        "\n",
        "    def train(self, data_generator):\n",
        "        \"\"\"\n",
        "        Train the LSTM on a dataset using backpropagation through time.\n",
        "\n",
        "        Args:\n",
        "            data_generator: An instance of DataGenerator containing the training data.\n",
        "\n",
        "        Returns:\n",
        "            None\n",
        "        \"\"\"\n",
        "        iter_num = 0\n",
        "        # stopping criterion for training\n",
        "        threshold = 46\n",
        "        smooth_loss = -np.log(1.0 / data_generator.vocab_size) * self.sequence_length  # initialize loss\n",
        "        while (smooth_loss > threshold):\n",
        "            # initialize hidden state at the beginning of each sequence\n",
        "            if data_generator.pointer == 0:\n",
        "                c_prev = np.zeros((self.hidden_size, 1))\n",
        "                a_prev = np.zeros((self.hidden_size, 1))\n",
        "\n",
        "            # get a batch of inputs and targets\n",
        "            inputs, targets = data_generator.next_batch()\n",
        "\n",
        "            # forward pass\n",
        "            X, y_pred, c, f, i, o, cc, a   = self.forward(inputs, c_prev, a_prev)\n",
        "\n",
        "            # backward pass\n",
        "            self.backward( X, targets, y_pred, c_prev, a_prev, c, f, i, o, cc, a)\n",
        "\n",
        "            # calculate and update loss\n",
        "            loss = self.loss(y_pred, targets)\n",
        "            self.adamw()\n",
        "            smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
        "            # update previous hidden state for the next batch\n",
        "            a_prev = a[self.sequence_length - 1]\n",
        "            c_prev = c[self.sequence_length - 1]\n",
        "            # print progress every 1000 iterations\n",
        "            if iter_num % 1000 == 0:\n",
        "                self.learning_rate *= 0.99\n",
        "                sample_idx = self.sample(c_prev, a_prev, inputs[0, :], 200)\n",
        "                sample_idx = [i for i in sample_idx]\n",
        "                print(sample_idx)\n",
        "                print(idx for idx in sample_idx)\n",
        "                print(data_generator.idx_to_char[int(idx)] for idx in sample_idx)\n",
        "                print(''.join(data_generator.idx_to_char[int(idx)] for idx in sample_idx))  # adaptação para cupy\n",
        "                print(\"\\n\\niter :%d, loss:%f\" % (iter_num, smooth_loss))\n",
        "            iter_num += 1\n",
        "\n",
        "\n",
        "    def sample(self, c_prev, a_prev, seed_idx, n):\n",
        "        \"\"\"\n",
        "        Sample a sequence of integers from the model.\n",
        "\n",
        "        Args:\n",
        "            c_prev (numpy.ndarray): Previous cell state, a numpy array of shape (hidden_size, 1).\n",
        "            a_prev (numpy.ndarray): Previous hidden state, a numpy array of shape (hidden_size, 1).\n",
        "            seed_idx (numpy.ndarray): Seed letter from the first time step, a numpy array of shape (vocab_size, 1).\n",
        "            n (int): Number of characters to generate.\n",
        "\n",
        "        Returns:\n",
        "            list: A list of integers representing the generated sequence.\n",
        "\n",
        "        \"\"\"\n",
        "        # initialize input and seed_idx\n",
        "        x = np.zeros((self.vocab_size, 1))\n",
        "        # convert one-hot encoding to integer index\n",
        "        seed_idx = np.argmax(seed_idx, axis=-1)\n",
        "\n",
        "        # set the seed letter as the input for the first time step\n",
        "        x[seed_idx] = 1\n",
        "\n",
        "        # generate sequence of characters\n",
        "        idxes = []\n",
        "        c = np.copy(c_prev)\n",
        "        a = np.copy(a_prev)\n",
        "        for t in range(n):\n",
        "            # compute the hidden state and cell state\n",
        "            concat = np.vstack((a, x))\n",
        "            i = self.sigmoid(np.dot(self.Wi, concat) + self.bi)\n",
        "            f = self.sigmoid(np.dot(self.Wf, concat) + self.bf)\n",
        "            cc = np.tanh(np.dot(self.Wc, concat) + self.bc)\n",
        "            c = f * c + i * cc\n",
        "            o = self.sigmoid(np.dot(self.Wo, concat) + self.bo)\n",
        "            a = o * np.tanh(c)\n",
        "\n",
        "            # compute the output probabilities\n",
        "            y = self.softmax(np.dot(self.Wy, a) + self.by)\n",
        "\n",
        "            # sample the next character from the output probabilitiesidx = np.random.choice(range(self.vocab_size), size=1, p=y.ravel())  # adaptação para cupy\n",
        "            idx = np.random.choice(list(range(self.vocab_size)), size=1, p=y.ravel())\n",
        "\n",
        "            # set the input for the next time step\n",
        "            x = np.zeros((self.vocab_size, 1))\n",
        "            x[idx] = 1\n",
        "\n",
        "            # append the sampled character to the sequence\n",
        "            idxes.append(idx)\n",
        "\n",
        "        # return the generated sequence\n",
        "        return idxes\n",
        "\n",
        "\n",
        "    def predict(self, data_generator, start, n):\n",
        "        \"\"\"\n",
        "        Generate a sequence of n characters using the trained LSTM model, starting from the given start sequence.\n",
        "\n",
        "        Args:\n",
        "        - data_generator: an instance of DataGenerator\n",
        "        - start: a string containing the start sequence\n",
        "        - n: an integer indicating the length of the generated sequence\n",
        "\n",
        "        Returns:\n",
        "        - txt: a string containing the generated sequence\n",
        "        \"\"\"\n",
        "        # initialize input sequence\n",
        "        x = np.zeros((self.vocab_size, 1))\n",
        "        chars = [ch for ch in start]\n",
        "        idxes = []\n",
        "        for i in range(len(chars)):\n",
        "            idx = data_generator.char_to_idx[chars[i]]\n",
        "            x[idx] = 1\n",
        "            idxes.append(idx)\n",
        "        # initialize cell state and hidden state\n",
        "        a = np.zeros((self.hidden_size, 1))\n",
        "        c = np.zeros((self.hidden_size, 1))\n",
        "\n",
        "        # generate new sequence of characters\n",
        "        for t in range(n):\n",
        "            # compute the hidden state and cell state\n",
        "            concat = np.vstack((a, x))\n",
        "            i = self.sigmoid(np.dot(self.Wi, concat) + self.bi)\n",
        "            f = self.sigmoid(np.dot(self.Wf, concat) + self.bf)\n",
        "            cc = np.tanh(np.dot(self.Wc, concat) + self.bc)\n",
        "            c = f * c + i * cc\n",
        "            o = self.sigmoid(np.dot(self.Wo, concat) + self.bo)\n",
        "            a = o * np.tanh(c)\n",
        "            # compute the output probabilities\n",
        "            y_pred = self.softmax(np.dot(self.Wy, a) + self.by)\n",
        "            # sample the next character from the output probabilities\n",
        "            idx = np.random.choice(range(self.vocab_size), size=1, p=y.ravel())\n",
        "            x = np.zeros((self.vocab_size, 1))\n",
        "            x[idx] = 1\n",
        "            idxes.append(idx)\n",
        "\n",
        "        txt = ''.join(data_generator.idx_to_char[i] for i in idxes)\n",
        "        txt.replace('\\n',\"\")\n",
        "        return txt\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "2sTZRS4zKbzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"alert alert-block alert-danger\">  \n",
        "<b>Warning:</b> This takes a very long time to converge\n",
        "</div>"
      ],
      "metadata": {
        "id": "hkNlukx2KbzS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequence_length = 28\n",
        "#read text from the \"input.txt\" file\n",
        "data_generator = DataGenerator('/kaggle/input/machado-de-assis/raw/txt/teatro/naoConsultesMedico.txt', sequence_length)\n",
        "lstm =  LSTM(hidden_size=1000, vocab_size=data_generator.vocab_size,sequence_length=sequence_length,learning_rate=1e-3)\n",
        "lstm.train(data_generator)"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2023-03-07T18:36:21.050146Z",
          "iopub.execute_input": "2023-03-07T18:36:21.050564Z"
        },
        "trusted": true,
        "id": "HhaqpyefKbzT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "891282c9-d43e-4aee-bfb1-09ba4b821a75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[array([34]), array([33]), array([38]), array([63]), array([9]), array([54]), array([71]), array([67]), array([31]), array([38]), array([72]), array([66]), array([20]), array([17]), array([48]), array([11]), array([35]), array([8]), array([73]), array([45]), array([15]), array([47]), array([71]), array([4]), array([54]), array([51]), array([30]), array([43]), array([13]), array([28]), array([25]), array([46]), array([48]), array([60]), array([15]), array([55]), array([43]), array([70]), array([21]), array([4]), array([70]), array([53]), array([63]), array([22]), array([28]), array([37]), array([15]), array([44]), array([54]), array([64]), array([56]), array([26]), array([48]), array([19]), array([65]), array([57]), array([63]), array([55]), array([42]), array([25]), array([29]), array([14]), array([47]), array([38]), array([0]), array([74]), array([29]), array([70]), array([25]), array([52]), array([31]), array([53]), array([65]), array([21]), array([62]), array([22]), array([5]), array([41]), array([43]), array([37]), array([58]), array([64]), array([47]), array([61]), array([35]), array([45]), array([18]), array([26]), array([12]), array([46]), array([36]), array([11]), array([72]), array([24]), array([72]), array([44]), array([33]), array([9]), array([25]), array([57]), array([53]), array([25]), array([48]), array([20]), array([21]), array([42]), array([41]), array([36]), array([50]), array([62]), array([21]), array([24]), array([19]), array([69]), array([28]), array([54]), array([20]), array([71]), array([70]), array([66]), array([50]), array([59]), array([1]), array([1]), array([14]), array([6]), array([11]), array([65]), array([71]), array([49]), array([30]), array([67]), array([2]), array([34]), array([38]), array([48]), array([39]), array([47]), array([1]), array([28]), array([17]), array([17]), array([32]), array([56]), array([25]), array([27]), array([27]), array([8]), array([9]), array([63]), array([42]), array([44]), array([37]), array([44]), array([6]), array([40]), array([51]), array([4]), array([64]), array([36]), array([39]), array([55]), array([36]), array([9]), array([48]), array([57]), array([41]), array([68]), array([50]), array([70]), array([4]), array([23]), array([72]), array([27]), array([62]), array([69]), array([34]), array([66]), array([7]), array([64]), array([53]), array([70]), array([12]), array([24]), array([41]), array([10]), array([73]), array([69]), array([30]), array([22]), array([29]), array([54]), array([68]), array([48]), array([56]), array([14]), array([35]), array([70]), array([23]), array([40])]\n",
            "<generator object LSTM.train.<locals>.<genexpr> at 0x7b6b12dd5e70>\n",
            "<generator object LSTM.train.<locals>.<genexpr> at 0x7b6b12dd5e70>\n",
            "pTBhd;PrNBVlFfIõUíôO(MPQ;oLÁ!JRsI,(—Á)çQ)ChzJj(à; ÉmIÃéDh—üR-aMBxE-)R.NCéçãzeuÁjá M:UOvm\n",
            "sqõVúVàTdRDCRIFçüuq?ãçúÃgJ;FP)l?nGGaXõéPÀLrêpBIóMGJffSÉRttídhüàjàXioQ qó—qdIDub?)Q\"VtãgplA C)\n",
            "úuâôgLz-;bIÉaU)\"i\n",
            "\n",
            "\n",
            "iter :0, loss:120.889677\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-4fe2c38dfa1f>:364: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  print(''.join(data_generator.idx_to_char[int(idx)] for idx in sample_idx))  # adaptação para cupy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[array([14]), array([52]), array([12]), array([15]), array([52]), array([64]), array([7]), array([30]), array([9]), array([30]), array([45]), array([33]), array([14]), array([29]), array([15]), array([34]), array([34]), array([51]), array([59]), array([27]), array([51]), array([60]), array([64]), array([36]), array([41]), array([5]), array([54]), array([67]), array([64]), array([51]), array([52]), array([12]), array([15]), array([67]), array([26]), array([40]), array([69]), array([16]), array([5]), array([41]), array([26]), array([1]), array([64]), array([46]), array([5]), array([46]), array([12]), array([26]), array([7]), array([14]), array([13]), array([64]), array([67]), array([41]), array([67]), array([26]), array([68]), array([34]), array([14]), array([18]), array([34]), array([13]), array([64]), array([5]), array([18]), array([27]), array([67]), array([14]), array([46]), array([64]), array([26]), array([5]), array([64]), array([46]), array([64]), array([5]), array([64]), array([31]), array([51]), array([46]), array([52]), array([52]), array([64]), array([7]), array([64]), array([53]), array([41]), array([53]), array([30]), array([45]), array([33]), array([7]), array([64]), array([36]), array([41]), array([32]), array([64]), array([71]), array([51]), array([64]), array([64]), array([26]), array([51]), array([40]), array([27]), array([51]), array([64]), array([14]), array([64]), array([53]), array([14]), array([59]), array([62]), array([14]), array([12]), array([64]), array([59]), array([27]), array([5]), array([66]), array([64]), array([5]), array([64]), array([65]), array([64]), array([65]), array([14]), array([65]), array([27]), array([14]), array([64]), array([53]), array([7]), array([26]), array([40]), array([41]), array([64]), array([55]), array([64]), array([36]), array([41]), array([67]), array([64]), array([34]), array([40]), array([46]), array([64]), array([14]), array([64]), array([14]), array([64]), array([45]), array([41]), array([0]), array([14]), array([46]), array([64]), array([9]), array([5]), array([64]), array([59]), array([51]), array([26]), array([64]), array([5]), array([59]), array([51]), array([52]), array([12]), array([31]), array([40]), array([5]), array([66]), array([14]), array([5]), array([64]), array([14]), array([58]), array([64]), array([36]), array([67]), array([14]), array([64]), array([36]), array([41]), array([5]), array([64]), array([36]), array([41]), array([5]), array([64]), array([36]), array([41]), array([5]), array([64]), array([65]), array([13]), array([64]), array([7]), array([59])]\n",
            "<generator object LSTM.train.<locals>.<genexpr> at 0x7b6b12dd6110>\n",
            "<generator object LSTM.train.<locals>.<genexpr> at 0x7b6b12dd6110>\n",
            "a.\n",
            "(. ALdLOTa-(pponto, que;r o.\n",
            "(rmigceumG ses\n",
            "mAa! rurmbpavp! evtras me s e Nos.. A CuCLOTA quS Po  moito a Canãa\n",
            " ntel e é éaéta CAmiu — qur pis a a Ouxas de nom eno.\n",
            "Nielae aá qra que que que é! An\n",
            "\n",
            "\n",
            "iter :1000, loss:92.302402\n",
            "[array([27]), array([62]), array([14]), array([64]), array([46]), array([64]), array([59]), array([62]), array([51]), array([64]), array([14]), array([64]), array([69]), array([40]), array([59]), array([26]), array([14]), array([64]), array([65]), array([46]), array([69]), array([5]), array([69]), array([68]), array([14]), array([40]), array([46]), array([5]), array([46]), array([66]), array([51]), array([67]), array([64]), array([59]), array([51]), array([5]), array([52]), array([12]), array([53]), array([7]), array([72]), array([7]), array([30]), array([53]), array([7]), array([31]), array([33]), array([74]), array([64]), array([55]), array([64]), array([72]), array([41]), array([64]), array([13]), array([12]), array([53]), array([7]), array([72]), array([57]), array([64]), array([53]), array([7]), array([31]), array([33]), array([74]), array([64]), array([55]), array([64]), array([7]), array([40]), array([46]), array([12]), array([65]), array([40]), array([64]), array([26]), array([64]), array([26]), array([40]), array([67]), array([5]), array([9]), array([60]), array([52]), array([12]), array([47]), array([26]), array([1]), array([53]), array([7]), array([72]), array([30]), array([45]), array([33]), array([7]), array([64]), array([55]), array([64]), array([45]), array([46]), array([27]), array([14]), array([64]), array([44]), array([27]), array([65]), array([16]), array([5]), array([50]), array([64]), array([59]), array([26]), array([14]), array([64]), array([5]), array([41]), array([27]), array([9]), array([59]), array([22]), array([52]), array([64]), array([45]), array([5]), array([27]), array([36]), array([41]), array([5]), array([46]), array([52]), array([52]), array([64]), array([32]), array([64]), array([51]), array([46]), array([64]), array([51]), array([64]), array([36]), array([62]), array([51]), array([64]), array([14]), array([64]), array([27]), array([14]), array([46]), array([63]), array([26]), array([40]), array([66]), array([51]), array([46]), array([52]), array([64]), array([45]), array([64]), array([13]), array([64]), array([34]), array([40]), array([40]), array([54]), array([64]), array([18]), array([5]), array([66]), array([63]), array([14]), array([9]), array([46]), array([67]), array([5]), array([64]), array([66]), array([14]), array([46]), array([9]), array([51]), array([64]), array([68]), array([51]), array([66]), array([27]), array([67]), array([64]), array([34]), array([5]), array([59]), array([27]), array([26]), array([14]), array([67]), array([64]), array([26]), array([5]), array([46]), array([27])]\n",
            "<generator object LSTM.train.<locals>.<genexpr> at 0x7b6b12dd5c40>\n",
            "<generator object LSTM.train.<locals>.<genexpr> at 0x7b6b12dd5c40>\n",
            "tãa s não a ginma ésgegbaiseslor noe.\n",
            "CAVALCANTE — Vu !\n",
            "CAVD CANTE — Ais\n",
            "éi m mired,.\n",
            "MmGCAVLOTA — Osta àtéce? nma eutdnz. Oetques.. S os o qão a tashmilos. O ! pii; velhadsre lasdo boltr pentmar mest\n",
            "\n",
            "\n",
            "iter :2000, loss:71.559483\n",
            "[array([1]), array([30]), array([57]), array([74]), array([30]), array([7]), array([48]), array([57]), array([74]), array([64]), array([55]), array([64]), array([71]), array([14]), array([67]), array([69]), array([14]), array([64]), array([27]), array([67]), array([9]), array([51]), array([12]), array([40]), array([46]), array([5]), array([64]), array([32]), array([0]), array([31]), array([59]), array([51]), array([64]), array([27]), array([51]), array([18]), array([5]), array([52]), array([64]), array([14]), array([14]), array([64]), array([17]), array([5]), array([59]), array([27]), array([51]), array([68]), array([46]), array([5]), array([37]), array([64]), array([14]), array([18]), array([14]), array([46]), array([27]), array([5]), array([13]), array([52]), array([12]), array([47]), array([74]), array([64]), array([7]), array([41]), array([3]), array([19]), array([21]), array([32]), array([60]), array([64]), array([16]), array([14]), array([59]), array([9]), array([67]), array([5]), array([66]), array([5]), array([67]), array([64]), array([27]), array([14]), array([67]), array([27]), array([16]), array([41]), array([67]), array([54]), array([64]), array([26]), array([14]), array([59]), array([14]), array([64]), array([41]), array([26]), array([65]), array([12]), array([59]), array([51]), array([46]), array([27]), array([51]), array([46]), array([5]), array([64]), array([3]), array([69]), array([67]), array([40]), array([67]), array([14]), array([64]), array([53]), array([51]), array([67]), array([14]), array([40]), array([9]), array([51]), array([64]), array([5]), array([64]), array([65]), array([64]), array([9]), array([51]), array([59]), array([9]), array([51]), array([60]), array([64]), array([47]), array([14]), array([59]), array([27]), array([51]), array([70]), array([54]), array([64]), array([5]), array([64]), array([1]), array([41]), array([35]), array([27]), array([40]), array([51]), array([60]), array([64]), array([1]), array([51]), array([69]), array([14]), array([64]), array([53]), array([40]), array([26]), array([5]), array([64]), array([27]), array([5]), array([16]), array([40]), array([41]), array([14]), array([12]), array([47]), array([52]), array([71]), array([7]), array([57]), array([16]), array([30]), array([7]), array([33]), array([57]), array([64]), array([34]), array([5]), array([46]), array([69]), array([40]), array([34]), array([51]), array([29]), array([9]), array([5]), array([64]), array([5]), array([41]), array([27]), array([9]), array([5]), array([16]), array([14]), array([50]), array([60])]\n",
            "<generator object LSTM.train.<locals>.<genexpr> at 0x7b6b12dd61f0>\n",
            "<generator object LSTM.train.<locals>.<genexpr> at 0x7b6b12dd61f0>\n",
            "GLDELAIDE — Parga trdo\n",
            "ise SxNno tove. aa fentobsej avaste!.\n",
            "ME AuHÃçS, candreler tartcur; mana umé\n",
            "nostose Hgrira Coraido e é dondo, Manto); e GuUtio, Goga Cime teciua\n",
            "M.PADcLATD pesgipo-de eutdeca?,\n",
            "\n",
            "\n",
            "iter :3000, loss:61.503614\n",
            "[array([41]), array([5]), array([52]), array([12]), array([53]), array([7]), array([72]), array([7]), array([30]), array([53]), array([7]), array([31]), array([33]), array([74]), array([64]), array([15]), array([14]), array([37]), array([69]), array([67]), array([5]), array([52]), array([52]), array([12]), array([53]), array([7]), array([72]), array([7]), array([30]), array([53]), array([7]), array([31]), array([33]), array([74]), array([64]), array([55]), array([64]), array([45]), array([41]), array([64]), array([9]), array([40]), array([9]), array([5]), array([64]), array([5]), array([12]), array([53]), array([7]), array([72]), array([7]), array([30]), array([53]), array([7]), array([31]), array([33]), array([74]), array([64]), array([55]), array([64]), array([45]), array([64]), array([34]), array([67]), array([40]), array([22]), array([40]), array([54]), array([64]), array([26]), array([14]), array([46]), array([46]), array([27]), array([5]), array([27]), array([5]), array([64]), array([14]), array([64]), array([25]), array([5]), array([59]), array([27]), array([51]), array([16]), array([40]), array([67]), array([5]), array([12]), array([64]), array([74]), array([64]), array([5]), array([67]), array([5]), array([44]), array([64]), array([16]), array([67]), array([51]), array([46]), array([27]), array([51]), array([60]), array([64]), array([27]), array([5]), array([66]), array([16]), array([5]), array([52]), array([64]), array([72]), array([51]), array([40]), array([26]), array([41]), array([52]), array([12]), array([57]), array([52]), array([64]), array([30]), array([74]), array([45]), array([53]), array([43]), array([57]), array([48]), array([7]), array([64]), array([55]), array([64]), array([45]), array([59]), array([69]), array([41]), array([59]), array([50]), array([64]), array([74]), array([64]), array([66]), array([5]), array([62]), array([67]), array([14]), array([52]), array([12]), array([57]), array([52]), array([64]), array([30]), array([74]), array([45]), array([53]), array([43]), array([57]), array([48]), array([7]), array([64]), array([55]), array([64]), array([47]), array([68]), array([40]), array([41]), array([64]), array([36]), array([41]), array([5]), array([64]), array([26]), array([51]), array([16]), array([14]), array([16]), array([40]), array([26]), array([52]), array([52]), array([64]), array([57]), array([52]), array([64]), array([30]), array([9]), array([5]), array([16]), array([7]), array([57]), array([16]), array([14]), array([64]), array([55]), array([64]), array([7]), array([64]), array([17])]\n",
            "<generator object LSTM.train.<locals>.<genexpr> at 0x7b6b12dd5af0>\n",
            "<generator object LSTM.train.<locals>.<genexpr> at 0x7b6b12dd5af0>\n",
            "ue.\n",
            "CAVALCANTE (ajgre..\n",
            "CAVALCANTE — Ou dide e\n",
            "CAVALCANTE — O prizi; masstete a Rentocire\n",
            " E ereà crosto, telce. Voimu.\n",
            "D. LEOCÁDIA — Ongun? E leãra.\n",
            "D. LEOCÁDIA — Mbiu que mocacim.. D. LdecADca — A f\n",
            "\n",
            "\n",
            "iter :4000, loss:56.126734\n",
            "[array([21]), array([40]), array([46]), array([60]), array([64]), array([59]), array([51]), array([64]), array([72]), array([58]), array([64]), array([41]), array([26]), array([64]), array([34]), array([51]), array([41]), array([26]), array([14]), array([29]), array([46]), array([5]), array([50]), array([52]), array([12]), array([57]), array([52]), array([64]), array([30]), array([74]), array([45]), array([53]), array([43]), array([57]), array([48]), array([7]), array([64]), array([55]), array([64]), array([31]), array([62]), array([51]), array([64]), array([46]), array([51]), array([66]), array([63]), array([14]), array([67]), array([40]), array([67]), array([64]), array([16]), array([51]), array([46]), array([64]), array([16]), array([51]), array([66]), array([65]), array([29]), array([66]), array([14]), array([64]), array([9]), array([5]), array([46]), array([64]), array([16]), array([58]), array([46]), array([67]), array([51]), array([67]), array([51]), array([64]), array([26]), array([5]), array([46]), array([64]), array([17]), array([41]), array([40]), array([46]), array([60]), array([64]), array([16]), array([51]), array([16]), array([2]), array([67]), array([67]), array([14]), array([9]), array([5]), array([59]), array([63]), array([58]), array([13]), array([12]), array([57]), array([52]), array([64]), array([30]), array([74]), array([45]), array([53]), array([43]), array([57]), array([48]), array([7]), array([64]), array([55]), array([64]), array([38]), array([51]), array([27]), array([64]), array([36]), array([41]), array([5]), array([64]), array([59]), array([51]), array([64]), array([30]), array([5]), array([46]), array([27]), array([58]), array([52]), array([64]), array([55]), array([64]), array([20]), array([5]), array([46]), array([64]), array([65]), array([41]), array([64]), array([17]), array([5]), array([67]), array([40]), array([9]), array([51]), array([54]), array([64]), array([14]), array([16]), array([37]), array([51]), array([64]), array([17]), array([5]), array([66]), array([51]), array([64]), array([9]), array([51]), array([37]), array([26]), array([64]), array([65]), array([64]), array([36]), array([41]), array([5]), array([64]), array([59]), array([62]), array([51]), array([64]), array([16]), array([63]), array([65]), array([64]), array([59]), array([62]), array([51]), array([64]), array([26]), array([5]), array([64]), array([21]), array([67]), array([14]), array([59]), array([63]), array([26]), array([64]), array([59]), array([63]), array([51]), array([64]), array([66]), array([5]), array([46]), array([29])]\n",
            "<generator object LSTM.train.<locals>.<genexpr> at 0x7b6b12dd5c40>\n",
            "<generator object LSTM.train.<locals>.<genexpr> at 0x7b6b12dd5c40>\n",
            "çis, no Vá um pouma-se?.\n",
            "D. LEOCÁDIA — Não solharir cos colé-la des cásroro mes fuis, cocêrradenhá!\n",
            "D. LEOCÁDIA — Bot que no Lestá. — Fes éu ferido; acjo felo dojm é que não ché não me çranhm nho les-\n",
            "\n",
            "\n",
            "iter :5000, loss:53.038840\n",
            "[array([51]), array([67]), array([14]), array([27]), array([5]), array([67]), array([65]), array([64]), array([9]), array([51]), array([64]), array([34]), array([8]), array([67]), array([5]), array([41]), array([52]), array([12]), array([57]), array([52]), array([64]), array([53]), array([7]), array([25]), array([30]), array([45]), array([33]), array([7]), array([64]), array([55]), array([64]), array([71]), array([51]), array([67]), array([46]), array([62]), array([51]), array([64]), array([46]), array([5]), array([41]), array([64]), array([51]), array([46]), array([12]), array([14]), array([46]), array([27]), array([62]), array([51]), array([60]), array([64]), array([5]), array([9]), array([14]), array([64]), array([57]), array([52]), array([64]), array([53]), array([14]), array([5]), array([66]), array([51]), array([18]), array([14]), array([13]), array([12]), array([53]), array([7]), array([72]), array([7]), array([30]), array([53]), array([7]), array([31]), array([33]), array([74]), array([64]), array([55]), array([64]), array([45]), array([41]), array([52]), array([64]), array([31]), array([62]), array([51]), array([52]), array([64]), array([20]), array([51]), array([26]), array([5]), array([59]), array([60]), array([64]), array([68]), array([5]), array([67]), array([51]), array([64]), array([59]), array([14]), array([59]), array([69]), array([51]), array([52]), array([12]), array([53]), array([7]), array([72]), array([7]), array([30]), array([53]), array([7]), array([31]), array([33]), array([74]), array([64]), array([55]), array([64]), array([72]), array([5]), array([40]), array([64]), array([18]), array([14]), array([59]), array([27]), array([14]), array([67]), array([64]), array([44]), array([64]), array([41]), array([46]), array([14]), array([18]), array([14]), array([64]), array([59]), array([62]), array([51]), array([60]), array([64]), array([5]), array([26]), array([27]), array([40]), array([51]), array([64]), array([9]), array([40]), array([26]), array([50]), array([12]), array([53]), array([7]), array([72]), array([7]), array([30]), array([53]), array([7]), array([31]), array([33]), array([74]), array([64]), array([55]), array([64]), array([56]), array([64]), array([14]), array([26]), array([64]), array([5]), array([41]), array([26]), array([2]), array([12]), array([18]), array([14]), array([59]), array([63]), array([14]), array([61]), array([52]), array([64]), array([23]), array([41]), array([64]), array([18]), array([5]), array([64]), array([34]), array([14]), array([67]), array([14]), array([64]), array([16])]\n",
            "<generator object LSTM.train.<locals>.<genexpr> at 0x7b6b12dd62d0>\n",
            "<generator object LSTM.train.<locals>.<genexpr> at 0x7b6b12dd62d0>\n",
            "orateré do píreu.\n",
            "D. CARLOTA — Porsão seu os\n",
            "astão, eda D. Caelova!\n",
            "CAVALCANTE — Ou. Não. Fomen, bero nango.\n",
            "CAVALCANTE — Vei vantar à usava não, emtio dim?\n",
            "CAVALCANTE — É am eumê\n",
            "vanha:. \"u ve para c\n",
            "\n",
            "\n",
            "iter :6000, loss:50.830898\n",
            "[array([14]), array([64]), array([5]), array([26]), array([5]), array([64]), array([17]), array([40]), array([46]), array([14]), array([64]), array([5]), array([46]), array([46]), array([5]), array([59]), array([63]), array([5]), array([26]), array([14]), array([64]), array([46]), array([14]), array([13]), array([64]), array([71]), array([14]), array([67]), array([40]), array([64]), array([18]), array([14]), array([40]), array([27]), array([51]), array([64]), array([21]), array([62]), array([51]), array([64]), array([5]), array([46]), array([5]), array([64]), array([37]), array([41]), array([27]), array([67]), array([5]), array([67]), array([60]), array([64]), array([14]), array([64]), array([34]), array([67]), array([5]), array([44]), array([14]), array([29]), array([46]), array([5]), array([54]), array([64]), array([26]), array([51]), array([40]), array([64]), array([34]), array([40]), array([72]), array([41]), array([52]), array([12]), array([47]), array([5]), array([67]), array([5]), array([64]), array([65]), array([12]), array([46]), array([14]), array([64]), array([66]), array([14]), array([9]), array([14]), array([29]), array([26]), array([14]), array([64]), array([51]), array([14]), array([34]), array([14]), array([74]), array([41]), array([52]), array([12]), array([47]), array([7]), array([1]), array([7]), array([30]), array([3]), array([19]), array([74]), array([32]), array([64]), array([55]), array([64]), array([33]), array([40]), array([46]), array([52]), array([64]), array([45]), array([41]), array([64]), array([26]), array([14]), array([64]), array([46]), array([41]), array([46]), array([41]), array([67]), array([14]), array([59]), array([14]), array([9]), array([5]), array([54]), array([64]), array([51]), array([46]), array([64]), array([63]), array([65]), array([64]), array([67]), array([14]), array([18]), array([51]), array([41]), array([64]), array([16]), array([51]), array([41]), array([9]), array([14]), array([64]), array([65]), array([0]), array([27]), array([67]), array([14]), array([60]), array([64]), array([67]), array([14]), array([16]), array([51]), array([67]), array([40]), array([59]), array([9]), array([51]), array([64]), array([5]), array([46]), array([27]), array([51]), array([64]), array([51]), array([12]), array([16]), array([14]), array([18]), array([14]), array([64]), array([5]), array([12]), array([69]), array([51]), array([46]), array([34]), array([14]), array([21]), array([14]), array([64]), array([5]), array([46]), array([27]), array([67]), array([14]), array([46]), array([64]), array([61])]\n",
            "<generator object LSTM.train.<locals>.<genexpr> at 0x7b6b12dd61f0>\n",
            "<generator object LSTM.train.<locals>.<genexpr> at 0x7b6b12dd61f0>\n",
            "a eme fisa essenhema sa! Pari vaito ção ese jutrer, a preàa-se; moi piVu.\n",
            "Mere é\n",
            "sa lada-ma oapaEu.\n",
            "MAGALHÃES — Tis. Ou ma susuranade; os hé ravou couda éxtra, racorindo esto o\n",
            "cava e\n",
            "gospaça estras :\n",
            "\n",
            "\n",
            "iter :7000, loss:48.810724\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-ba4cb713e18f>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdata_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/kaggle/input/machado-de-assis/raw/txt/teatro/naoConsultesMedico.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlstm\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mlstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-6-4fe2c38dfa1f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, data_generator)\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m            \u001b[0;31m# forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 344\u001b[0;31m            \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_prev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_prev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m            \u001b[0;31m# backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-4fe2c38dfa1f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X, c_prev, a_prev)\u001b[0m\n\u001b[1;32m    217\u001b[0m            \u001b[0;31m# concatenate the input and hidden state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m            \u001b[0mxt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m            \u001b[0mconcat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m            \u001b[0;31m# compute the forget gate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/cupy/_manipulation/join.py\u001b[0m in \u001b[0;36mvstack\u001b[0;34m(tup)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \"\"\"\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcupy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/cupy/_manipulation/join.py\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(tup, axis, out, dtype, casting)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mtup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lstm.predict(data_generator, \"c\", 150)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-07T18:21:54.396639Z",
          "iopub.status.idle": "2023-03-07T18:21:54.397972Z",
          "shell.execute_reply.started": "2023-03-07T18:21:54.397647Z",
          "shell.execute_reply": "2023-03-07T18:21:54.397684Z"
        },
        "trusted": true,
        "id": "8M1-suZoKbzT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"4\"></a>\n",
        "<h1 style='background:#FF9F00;border:0; color:black;\n",
        "    box-shadow: 10px 10px 5px 0px rgba(0,0,0,0.75);\n",
        "    transform: rotateX(10deg);\n",
        "    '><center style='color: #3E3D53;'>Thank you</center></h1>\n",
        "\n",
        "# Thank you\n",
        "\n",
        "**Thank you for going through this notebook**\n",
        "\n",
        "**If you have any suggestions please let me know**\n",
        "\n",
        "<a id=\"5\"></a>\n",
        "# References\n",
        "https://gist.github.com/karpathy/d4dee566867f8291f086\n",
        "\n",
        "https://arxiv.org/abs/1711.05101"
      ],
      "metadata": {
        "id": "1cCX3Se1KbzT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"padding:10px;\n",
        "            color:#333333;\n",
        "            margin:10px;\n",
        "            font-size:150%;\n",
        "            display:fill;\n",
        "            border-radius:1px;\n",
        "            border-style:solid;\n",
        "            border-color:#666666;\n",
        "            background-color:#F9F9F9;\n",
        "            overflow:hidden;\">\n",
        "    <center>\n",
        "        <a id='top'></a>\n",
        "        <b>Machine Learning From Scratch Series</b>\n",
        "    </center>\n",
        "    <br>\n",
        "    <ul>\n",
        "        <li>\n",
        "            <a href=\"https://www.kaggle.com/code/fareselmenshawii/linear-regression-from-scratch\" style=\"color:#0072B2\">1 - Linear Regression</a>\n",
        "        </li>\n",
        "        <li>\n",
        "            <a href=\"https://www.kaggle.com/code/fareselmenshawii/logistic-regression-from-scratch\" style=\"color:#0072B2\">2 -  Logistic Regression</a>\n",
        "        </li>\n",
        "        <li>\n",
        "            <a href=\"https://www.kaggle.com/code/fareselmenshawii/kmeans-from-scratch\" style=\"color:#0072B2\">3 - KMeans</a>\n",
        "        </li>\n",
        "        <li>\n",
        "            <a href=\"https://www.kaggle.com/code/fareselmenshawii/decision-tree-classifier-from-scratch\" style=\"color:#0072B2\">4 - Decision Trees</a>\n",
        "        </li>\n",
        "        <li>\n",
        "            <a href=\"https://www.kaggle.com/code/fareselmenshawii/random-forest-classifier-from-scratch\" style=\"color:#0072B2\">5 -  Random Forest</a>\n",
        "        </li>\n",
        "        <li>\n",
        "            <a href=\"https://www.kaggle.com/code/fareselmenshawii/knn-from-scratch\" style=\"color:#0072B2\">6 - KNearestNeighbor</a>\n",
        "        </li>\n",
        "        <li>\n",
        "            <a href=\"https://www.kaggle.com/code/fareselmenshawii/pca-from-scratch?scriptVersionId=121402593\" style=\"color:#0072B2\">7 - PCA</a>\n",
        "        </li>\n",
        "        <li>\n",
        "            <a href=\"https://www.kaggle.com/code/fareselmenshawii/svm-from-scratch\" style=\"color:#0072B2\">8 - SVM</a>\n",
        "        </li>\n",
        "        <li>\n",
        "            <a href=\"https://www.kaggle.com/code/fareselmenshawii/naive-bayes-from-scratch\" style=\"color:#0072B2\">9 - Naive Baye</a>\n",
        "        </li>\n",
        "        <li>\n",
        "            <a href=\"https://www.kaggle.com/code/fareselmenshawii/optimized-neural-network-from-scratch\" style=\"color:#0072B2\">10 - Optimized Neural Network</a>\n",
        "        </li>\n",
        "        <li>\n",
        "            <a href=\"https://www.kaggle.com/code/fareselmenshawii/neural-network-from-scratch\" style=\"color:#0072B2\">11 - Neural Network</a>\n",
        "        </li>\n",
        "        <li>\n",
        "            <a href=\"https://www.kaggle.com/code/fareselmenshawii/cnn-from-scratch\" style=\"color:#0072B2\">12 - CNN</a>\n",
        "        </li>\n",
        "        <li>\n",
        "            <a href=\"https://www.kaggle.com/code/fareselmenshawii/rnn-from-scratch\" style=\"color:#0072B2\">13 - RNN</a>\n",
        "        </li>\n",
        "        <li>\n",
        "            <a href=\"https://www.kaggle.com/code/fareselmenshawii/lstm-from-scratch\" style=\"color:#0072B2\">14 - LSTM</a>\n",
        "        </li>\n",
        "        <li>\n",
        "            <a href=\"https://www.kaggle.com/code/fareselmenshawii/gru-from-scratch\" style=\"color:#0072B2\">15 - GRU</a>\n",
        "        </li>\n",
        "    </ul>\n",
        "</div>"
      ],
      "metadata": {
        "id": "kWTeFlCqKbzT"
      }
    }
  ]
}