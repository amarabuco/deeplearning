{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amarabuco/deeplearning-2024.2/blob/main/GRU_From_Scratch_Machado_Br%C3%A1s_Cubas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'shakespeare-text:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F759820%2F1311864%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240930%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240930T230010Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D61643458ae5130ece74eff5eef9326e13aa2177169f0655636b32c16b3fd7585d08bef6df7adfcbbe00819d2fef3c0767cea37e78a0725299f76d9e7c32d32fb595d239b104e3cbe604039c9e4302cd453005aef938107283ab698c39eb046158d63bf61a2eaa9cbd67b48299f1aa3449f838f269f51eaffc293613614141e87731ba81804679ba4ee00ddc609b79fac63978c525aa46c3948a9a669fdd2e22344451a1c93a078af873f6e7ae0700747d162448307f1e19ed362f099ba342088134d76cb939e5772ab13cce9233cd46ee97b1c24326f0ce6ed9e1839c354032227189596d82e2a15d9ef77617923274bbc9ff8fc1f3fa299c87b74d5ed75965e'\n",
        "\n",
        "DATA_SOURCE_MAPPING = 'machado-de-assis:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F134301%2F319174%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20241003%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20241003T113414Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D16ed6c2f34e181423afa067781bc9e2ea47f6bd85466db594b50313e4ee67198153754d2b9a63979b57b636dd124a8e45e08f6f0369f17b1102fd048aa1a605a418e3f6b62b968fb0c470ed2ca46e428ff4269bdba31d093d778534ce894eeaf8f189e0d1960954473263f5f41f04bfd54b57794b829fe6b522eab1437383b130471c4832084ebb06d343d3d9be150faa153f329032b2954afaacdddf054115a6c21b3fb80f2a2a1268d903303956e0d20c47e47b10cfb5d573a7f05c168a973d57a77ff60a607c56b8c3d059ca2eb3660f9f549e76c530c9674566c7c79d1c79d6e90dafb836f63e71227b533049c5e89682d46a4a9d5e7530ca2b625a32c1a'\n",
        "\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "k9rMB5o2Ka4B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c13b7663-995f-454c-faa8-6ddb22e96890"
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading machado-de-assis, 26705568 bytes compressed\n",
            "[==================================================] 26705568 bytes downloaded\n",
            "Downloaded and uncompressed: machado-de-assis\n",
            "Data source import complete.\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "code",
      "source": [
        "open('/kaggle/input/machado-de-assis/raw/txt/romance/memoriasBras.txt', 'r').read().split('\\n')[:10] # Dataset é um conjunto de nomes de dinossauros\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYDx5NT9MpdD",
        "outputId": "b44fcd26-cf4a-4f1a-9ce4-6702e5df0ee9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Memórias Póstumas de Brás Cubas',\n",
              " '',\n",
              " 'Ao verme',\n",
              " 'que',\n",
              " 'primeiro roeu as frias carnes',\n",
              " 'do meu cadáver',\n",
              " 'dedico',\n",
              " 'como saudosa lembrança',\n",
              " 'estas',\n",
              " 'Memórias Póstumas']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"padding:10px;\n",
        "            color:#FF9F00;\n",
        "            margin:10px;\n",
        "            font-size:150%;\n",
        "            display:fill;\n",
        "            border-radius:1px;\n",
        "            border-style: solid;\n",
        "            border-color:#FF9F00;\n",
        "            background-color:#3E3D53;\n",
        "            overflow:hidden;\">\n",
        "    <center>\n",
        "        <a id='top'></a>\n",
        "        <b>Table of Contents</b>\n",
        "    </center>\n",
        "    <br>\n",
        "    <ul>\n",
        "        <li>\n",
        "            <a href=\"#1\">1 -  Overview and Imports</a>\n",
        "        </li>\n",
        "        <li>\n",
        "            <a href=\"#2\">2 - Data Preparation</a>\n",
        "        </li>\n",
        "        <li>\n",
        "            <a href=\"#3\">3 - GRU Implementation</a>\n",
        "        <li>\n",
        "            <a href=\"#4\">4 - Thank you</a>\n",
        "        </li>\n",
        "        <li>\n",
        "            <a href=\"#5\">5 - References</a>\n",
        "        </li>\n",
        "    </ul>\n",
        "</div>\n",
        "<a id=\"1\"></a>\n",
        "\n",
        "<h1 style='background:#FF9F00;border:0; color:black;\n",
        "    box-shadow: 10px 10px 5px 0px rgba(0,0,0,0.75);\n",
        "    transform: rotateX(10deg);\n",
        "    '><center style='color: #3E3D53;'>Overview and Imports</center></h1>\n",
        "    \n",
        "# Overview and Imports\n",
        "\n",
        "**Long Short-Term Memory (LSTM) networks are a type of Recurrent Neural Network (RNN) that are designed to handle sequential data, such as time series or natural language text. They achieve this by using a memory cell that can selectively remember or forget information at each time step of the input sequence, allowing the network to maintain a memory of previous inputs over a longer period of time.**\n",
        "\n",
        "**This notebook contains an implementation of an LSTM that can be used for language modeling. The self takes in a sequence of characters and outputs the probability distribution over the next character in the sequence. The network is trained on a corpus of text and then used to generate new text that has a similar distribution of characters as the training corpus.**"
      ],
      "metadata": {
        "id": "65YIEESjKa4F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "import cupy as np # substituir numpy por cupy para por usar GPU"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-12T11:31:01.213152Z",
          "iopub.execute_input": "2023-03-12T11:31:01.213591Z",
          "iopub.status.idle": "2023-03-12T11:31:01.219661Z",
          "shell.execute_reply.started": "2023-03-12T11:31:01.213555Z",
          "shell.execute_reply": "2023-03-12T11:31:01.218184Z"
        },
        "trusted": true,
        "id": "Yu2scckMKa4G"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DataGenerator:\n",
        "    \"\"\"\n",
        "    A class for reading and preprocessing text data.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, path: str, sequence_length: int):\n",
        "        \"\"\"\n",
        "        Initializes a DataReader object with the path to a text file and the desired sequence length.\n",
        "\n",
        "        Args:\n",
        "            path (str): The path to the text file.\n",
        "            sequence_length (int): The length of the sequences that will be fed to the self.\n",
        "        \"\"\"\n",
        "        with open(path, encoding='utf-8') as f:\n",
        "            # Read the contents of the file\n",
        "            self.data = f.read()\n",
        "\n",
        "        # Find all unique characters in the text\n",
        "        chars = list(set(self.data))\n",
        "\n",
        "        # Create dictionaries to map characters to indices and vice versa\n",
        "        self.char_to_idx = {ch: i for (i, ch) in enumerate(chars)}\n",
        "        self.idx_to_char = {i: ch for (i, ch) in enumerate(chars)}\n",
        "\n",
        "        # Store the size of the text data and the size of the vocabulary\n",
        "        self.data_size = len(self.data)\n",
        "        self.vocab_size = len(chars)\n",
        "\n",
        "        # Initialize the pointer that will be used to generate sequences\n",
        "        self.pointer = 0\n",
        "\n",
        "        # Store the desired sequence length\n",
        "        self.sequence_length = sequence_length\n",
        "\n",
        "\n",
        "    def next_batch(self):\n",
        "        \"\"\"\n",
        "        Generates a batch of input and target sequences.\n",
        "\n",
        "        Returns:\n",
        "            inputs_one_hot (np.ndarray): A numpy array with shape `(batch_size, vocab_size)` where each row is a one-hot encoded representation of a character in the input sequence.\n",
        "            targets (list): A list of integers that correspond to the indices of the characters in the target sequence, which is the same as the input sequence shifted by one position to the right.\n",
        "        \"\"\"\n",
        "        input_start = self.pointer\n",
        "        input_end = self.pointer + self.sequence_length\n",
        "\n",
        "        # Get the input sequence as a list of integers\n",
        "        inputs = [self.char_to_idx[ch] for ch in self.data[input_start:input_end]]\n",
        "\n",
        "        # One-hot encode the input sequence\n",
        "        inputs_one_hot = np.zeros((len(inputs), self.vocab_size))\n",
        "        inputs_one_hot[np.arange(len(inputs)), inputs] = 1\n",
        "\n",
        "        # Get the target sequence as a list of integers\n",
        "        targets = [self.char_to_idx[ch] for ch in self.data[input_start + 1:input_end + 1]]\n",
        "\n",
        "        # Update the pointer\n",
        "        self.pointer += self.sequence_length\n",
        "\n",
        "        # Reset the pointer if the next batch would exceed the length of the text data\n",
        "        if self.pointer + self.sequence_length + 1 >= self.data_size:\n",
        "            self.pointer = 0\n",
        "\n",
        "        return inputs_one_hot, targets"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-12T11:31:01.986084Z",
          "iopub.execute_input": "2023-03-12T11:31:01.986518Z",
          "iopub.status.idle": "2023-03-12T11:31:01.999737Z",
          "shell.execute_reply.started": "2023-03-12T11:31:01.98648Z",
          "shell.execute_reply": "2023-03-12T11:31:01.998155Z"
        },
        "trusted": true,
        "id": "CSddq6kqKa4H"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GRU:\n",
        "    \"\"\"\n",
        "    A class used to represent a Recurrent Neural Network (GRU).\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    hidden_size : int\n",
        "        The number of hidden units in the GR.\n",
        "    vocab_size : int\n",
        "        The size of the vocabulary used by the GRU.\n",
        "    sequence_length : int\n",
        "        The length of the input sequences fed to the GRU.\n",
        "    self.learning_rate : float\n",
        "        The learning rate used during training.\n",
        "\n",
        "    Methods\n",
        "    -------\n",
        "    __init__(hidden_size, vocab_size, sequence_length, self.learning_rate)\n",
        "        Initializes an instance of the GRU class.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, hidden_size, vocab_size, sequence_length, learning_rate):\n",
        "        \"\"\"\n",
        "        Initializes an instance of the GRU class.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        hidden_size : int\n",
        "            The number of hidden units in the GRU.\n",
        "        vocab_size : int\n",
        "            The size of the vocabulary used by the GRU.\n",
        "        sequence_length : int\n",
        "            The length of the input sequences fed to the GRU.\n",
        "        learning_rate : float\n",
        "            The learning rate used during training.\n",
        "        \"\"\"\n",
        "        # hyper parameters\n",
        "        self.hidden_size = hidden_size\n",
        "        self.vocab_size = vocab_size\n",
        "        self.sequence_length = sequence_length\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        # model parameters\n",
        "        self.Wz = np.random.uniform(-np.sqrt(1. / hidden_size), np.sqrt(1. / hidden_size),\n",
        "                                    (hidden_size, hidden_size + vocab_size))\n",
        "        self.bz = np.zeros((hidden_size, 1))\n",
        "\n",
        "        self.Wr = np.random.uniform(-np.sqrt(1. / hidden_size), np.sqrt(1. / hidden_size),\n",
        "                                    (hidden_size, hidden_size + vocab_size))\n",
        "        self.br = np.zeros((hidden_size, 1))\n",
        "\n",
        "        self.Wa = np.random.uniform(-np.sqrt(1. / hidden_size), np.sqrt(1. / hidden_size),\n",
        "                                    (hidden_size, hidden_size + vocab_size))\n",
        "        self.ba = np.zeros((hidden_size, 1))\n",
        "\n",
        "        self.Wy = np.random.uniform(-np.sqrt(1. / hidden_size), np.sqrt(1. / hidden_size),\n",
        "                                    (vocab_size, hidden_size))\n",
        "        self.by = np.zeros((vocab_size, 1))\n",
        "\n",
        "        # initialize gradients for each parameter\n",
        "        self.dWz, self.dWr, self.dWa, self.dWy = np.zeros_like(self.Wz), np.zeros_like(self.Wr), np.zeros_like(\n",
        "            self.Wa), np.zeros_like(self.Wy)\n",
        "        self.dbz, self.dbr, self.dba, self.dby = np.zeros_like(self.bz), np.zeros_like(self.br), np.zeros_like(\n",
        "            self.bz), np.zeros_like(self.by)\n",
        "\n",
        "        # initialize parameters for adamw optimizer\n",
        "        self.mWz = np.zeros_like(self.Wz)\n",
        "        self.vWz = np.zeros_like(self.Wz)\n",
        "        self.mWr = np.zeros_like(self.Wr)\n",
        "        self.vWr = np.zeros_like(self.Wr)\n",
        "        self.mWa = np.zeros_like(self.Wa)\n",
        "        self.vWa = np.zeros_like(self.Wa)\n",
        "        self.mWy = np.zeros_like(self.Wy)\n",
        "        self.vWy = np.zeros_like(self.Wy)\n",
        "        self.mbz = np.zeros_like(self.bz)\n",
        "        self.vbz = np.zeros_like(self.bz)\n",
        "        self.mbr = np.zeros_like(self.br)\n",
        "        self.vbr = np.zeros_like(self.br)\n",
        "        self.mba = np.zeros_like(self.ba)\n",
        "        self.vba = np.zeros_like(self.ba)\n",
        "        self.mby = np.zeros_like(self.by)\n",
        "        self.vby = np.zeros_like(self.by)\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        \"\"\"\n",
        "        Computes the sigmoid activation function for a given input array.\n",
        "\n",
        "        Parameters:\n",
        "            x (ndarray): Input array.\n",
        "\n",
        "        Returns:\n",
        "            ndarray: Array of the same shape as `x`, containing the sigmoid activation values.\n",
        "        \"\"\"\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def softmax(self, x):\n",
        "        \"\"\"\n",
        "        Computes the softmax activation function for a given input array.\n",
        "\n",
        "        Parameters:\n",
        "            x (ndarray): Input array.\n",
        "\n",
        "        Returns:\n",
        "            ndarray: Array of the same shape as `x`, containing the softmax activation values.\n",
        "        \"\"\"\n",
        "        # shift the input to prevent overflow when computing the exponentials\n",
        "        x = x - np.max(x)\n",
        "        # compute the exponentials of the shifted input\n",
        "        p = np.exp(x)\n",
        "        # normalize the exponentials by dividing by their sum\n",
        "        return p / np.sum(p)\n",
        "\n",
        "    def forward(self, X, c_prev, a_prev):\n",
        "        \"\"\"\n",
        "        Performs forward propagation for a simple GRU model.\n",
        "\n",
        "        Args:\n",
        "            X (numpy array): Input sequence, shape (sequence_length, input_size)\n",
        "            c_prev (numpy array): Previous cell state, shape (hidden_size, 1)\n",
        "            a_prev (numpy array): Previous hidden state, shape (hidden_size, 1)\n",
        "\n",
        "        Returns: X (numpy array): Input sequence, shape (sequence_length, input_size) c (dictionary): Cell state for\n",
        "        each time step, keys = time step, values = numpy array shape (hidden_size, 1) r (dictionary): Reset gate for\n",
        "        each time step, keys = time step, values = numpy array shape (hidden_size, 1) z (dictionary): Update gate for\n",
        "        each time step, keys = time step, values = numpy array shape (hidden_size, 1) cc (dictionary): Candidate cell\n",
        "        state for each time step, keys = time step, values = numpy array shape (hidden_size, 1) a (dictionary):\n",
        "        Hidden state for each time step, keys = time step, values = numpy array shape (hidden_size, 1) y_pred (\n",
        "        dictionary): Output probability vector for each time step, keys = time step, values = numpy array shape (\n",
        "        output_size, 1)\n",
        "        \"\"\"\n",
        "\n",
        "        # initialize dictionaries for backpropagation\n",
        "        # initialize dictionaries for backpropagation\n",
        "        r, z, c, cc, a, y_pred = {}, {}, {}, {}, {}, {}\n",
        "        c[-1] = np.copy(c_prev)  # store the initial cell state in the dictionary\n",
        "        a[-1] = np.copy(a_prev)  # store the initial hidden state in the dictionary\n",
        "\n",
        "        # iterate over each time step in the input sequence\n",
        "        for t in range(X.shape[0]):\n",
        "            # concatenate the input and hidden state\n",
        "            xt = X[t, :].reshape(-1, 1)\n",
        "            concat = np.vstack((a[t - 1], xt))\n",
        "\n",
        "            # compute the reset gate\n",
        "            r[t] = self.sigmoid(np.dot(self.Wr, concat) + self.br)\n",
        "\n",
        "            # compute the update gate\n",
        "            z[t] = self.sigmoid(np.dot(self.Wz, concat) + self.bz)\n",
        "\n",
        "            # compute the candidate cell state\n",
        "            cc[t] = np.tanh(np.dot(self.Wa, np.vstack((r[t] * a[t - 1], xt))) + self.ba)\n",
        "\n",
        "            # compute the cell state\n",
        "            c[t] = z[t] * cc[t] + (1 - z[t]) * c[t - 1]\n",
        "\n",
        "            # compute the hidden state\n",
        "            a[t] = c[t]\n",
        "\n",
        "            # compute the output probability vector\n",
        "            y_pred[t] = self.softmax(np.dot(self.Wy, a[t]) + self.by)\n",
        "\n",
        "        # return the output probability vectors, cell state, hidden state and gate vectors\n",
        "        return X, r, z, c, cc, a, y_pred\n",
        "\n",
        "    def backward(self, X, a_prev, c_prev, r, z, c, cc, a, y_pred, targets):\n",
        "        \"\"\"\n",
        "        Performs backward propagation through time for a GRU network.\n",
        "\n",
        "        Args:\n",
        "            X (numpy array): Input sequence, shape (sequence_length, input_size)\n",
        "            a_prev (numpy array): Previous hidden state, shape (hidden_size, 1)\n",
        "            r (dictionary): Reset gate for each time step, keys = time step, values = numpy array shape (hidden_size, 1)\n",
        "            z (dictionary): Update gate for each time step, keys = time step, values = numpy array shape (hidden_size, 1)\n",
        "            c (dictionary): Cell state for each time step, keys = time step, values = numpy array shape (hidden_size, 1)\n",
        "            cc (dictionary): Candidate cell state for each time step, keys = time step, values = numpy array shape (hidden_size, 1)\n",
        "            a (dictionary): Hidden state for each time step, keys = time step, values = numpy array shape (hidden_size, 1)\n",
        "            y_pred (dictionary): Output probability vector for each time step, keys = time step, values = numpy array shape (output_size, 1)\n",
        "            targets (numpy array): Target outputs for each time step, shape (sequence_length, output_size)\n",
        "\n",
        "        Returns:\n",
        "            None\n",
        "        \"\"\"\n",
        "        # Initialize gradients for hidden state\n",
        "        dc_next = np.zeros_like(c_prev)\n",
        "        da_next = np.zeros_like(a_prev)\n",
        "\n",
        "        # Iterate backwards through time steps\n",
        "        for t in reversed(range(X.shape[0])):\n",
        "            # compute the gradient of the output probability vector\n",
        "            dy = np.copy(y_pred[t])\n",
        "            dy[targets[t]] -= 1\n",
        "\n",
        "            # compute the gradient of the output layer weights and biases\n",
        "            self.dWy += np.dot(dy, a[t].T)\n",
        "            self.dby += dy\n",
        "\n",
        "            # compute the gradient of the hidden state\n",
        "            da = np.dot(self.Wy.T, dy) + da_next\n",
        "\n",
        "            # compute the gradient of the update gate\n",
        "            xt = X[t, :].reshape(-1, 1)\n",
        "            concat = np.vstack((a_prev, xt))\n",
        "            dz = da * (a[t] - c[t])\n",
        "            self.dWz += np.dot(dz, concat.T)\n",
        "            self.dbz += dz\n",
        "\n",
        "            # compute the gradient of the reset gate\n",
        "            dr = da * np.dot(self.Wz[:, :self.hidden_size].T, dz) * (1 - r[t]) * r[t]\n",
        "            self.dWr += np.dot(dr, concat.T)\n",
        "            self.dbr += dr\n",
        "\n",
        "            # compute the gradient of the current hidden state\n",
        "            da = np.dot(self.Wa[:, :self.hidden_size].T, dr) + np.dot(self.Wz[:, :self.hidden_size].T, dz)\n",
        "            self.dWa += np.dot(da * (1 - a[t]**2), concat.T)\n",
        "            self.dba += da * (1 - a[t]**2)\n",
        "\n",
        "            # compute the gradient of the input to the next hidden state\n",
        "            da_next = np.dot(self.Wr[:, :self.hidden_size].T, dr) \\\n",
        "                      + np.dot(self.Wz[:, :self.hidden_size].T, dz) \\\n",
        "                      + np.dot(self.Wa[:, :self.hidden_size].T, da)\n",
        "        # clip gradients to avoid exploding gradients\n",
        "        for grad in [self.dWz, self.dWr, self.dWa, self.dWy, self.dbz, self.dbr, self.dba, self.dby]:\n",
        "            np.clip(grad, -1, 1)\n",
        "\n",
        "    def loss(self, y_preds, targets):\n",
        "        \"\"\"\n",
        "        Computes the cross-entropy loss for a given sequence of predicted probabilities and true targets.\n",
        "\n",
        "        Parameters:\n",
        "            y_preds (ndarray): Array of shape (sequence_length, vocab_size) containing the predicted probabilities for each time step.\n",
        "            targets (ndarray): Array of shape (sequence_length, 1) containing the true targets for each time step.\n",
        "\n",
        "        Returns:\n",
        "            float: Cross-entropy loss.\n",
        "        \"\"\"\n",
        "        # calculate cross-entropy loss\n",
        "        return sum(-np.log(y_preds[t][targets[t], 0]) for t in range(self.sequence_length))\n",
        "\n",
        "    def adamw(self, beta1=0.9, beta2=0.999, epsilon=1e-8, L2_reg=1e-4):\n",
        "        \"\"\"\n",
        "        Updates the GRU's parameters using the AdamW optimization algorithm.\n",
        "        \"\"\"\n",
        "\n",
        "        # AdamW update for Wz\n",
        "        self.mWz = beta1 * self.mWz + (1 - beta1) * self.dWz\n",
        "        self.vWz = beta2 * self.vWz + (1 - beta2) * np.square(self.dWz)\n",
        "        m_hat = self.mWz / (1 - beta1)\n",
        "        v_hat = self.vWz / (1 - beta2)\n",
        "        self.Wz -= self.learning_rate * (m_hat / (np.sqrt(v_hat) + epsilon) + L2_reg * self.Wz)\n",
        "\n",
        "        # AdamW update for bu\n",
        "        self.mbz = beta1 * self.mbz + (1 - beta1) * self.dbz\n",
        "        self.vbz = beta2 * self.vbz + (1 - beta2) * np.square(self.dbz)\n",
        "        m_hat = self.mbz / (1 - beta1)\n",
        "        v_hat = self.vbz / (1 - beta2)\n",
        "        self.bz -= self.learning_rate * (m_hat / (np.sqrt(v_hat) + epsilon) + L2_reg * self.bz)\n",
        "\n",
        "        # AdamW update for Wr\n",
        "        self.mWr = beta1 * self.mWr + (1 - beta1) * self.dWr\n",
        "        self.vWr = beta2 * self.vWr + (1 - beta2) * np.square(self.dWr)\n",
        "        m_hat = self.mWr / (1 - beta1)\n",
        "        v_hat = self.vWr / (1 - beta2)\n",
        "        self.Wr -= self.learning_rate * (m_hat / (np.sqrt(v_hat) + epsilon) + L2_reg * self.Wr)\n",
        "\n",
        "        # AdamW update for br\n",
        "        self.mbr = beta1 * self.mbr + (1 - beta1) * self.dbr\n",
        "        self.vbr = beta2 * self.vbr + (1 - beta2) * np.square(self.dbr)\n",
        "        m_hat = self.mbr / (1 - beta1)\n",
        "        v_hat = self.vbr / (1 - beta2)\n",
        "        self.br -= self.learning_rate * (m_hat / (np.sqrt(v_hat) + epsilon) + L2_reg * self.br)\n",
        "\n",
        "        # AdamW update for Wa\n",
        "        self.mWa = beta1 * self.mWa + (1 - beta1) * self.dWa\n",
        "        self.vWa = beta2 * self.vWa + (1 - beta2) * np.square(self.dWa)\n",
        "        m_hat = self.mWa / (1 - beta1)\n",
        "        v_hat = self.vWa / (1 - beta2)\n",
        "        self.Wa -= self.learning_rate * (m_hat / (np.sqrt(v_hat) + epsilon) + L2_reg * self.Wa)\n",
        "\n",
        "        # AdamW update for br\n",
        "        self.mba = beta1 * self.mba + (1 - beta1) * self.dba\n",
        "        self.vba = beta2 * self.vba + (1 - beta2) * np.square(self.dba)\n",
        "        m_hat = self.mba / (1 - beta1)\n",
        "        v_hat = self.vba / (1 - beta2)\n",
        "        self.ba -= self.learning_rate * (m_hat / (np.sqrt(v_hat) + epsilon) + L2_reg * self.ba)\n",
        "\n",
        "        # AdamW update for Wy\n",
        "        self.mWy = beta1 * self.mWy + (1 - beta1) * self.dWy\n",
        "        self.vWy = beta2 * self.vWy + (1 - beta2) * np.square(self.dWy)\n",
        "        m_hat = self.mWy / (1 - beta1)\n",
        "        v_hat = self.vWy / (1 - beta2)\n",
        "        self.Wy -= self.learning_rate * (m_hat / (np.sqrt(v_hat) + epsilon) + L2_reg * self.Wy)\n",
        "\n",
        "        # AdamW update for by\n",
        "        self.mby = beta1 * self.mby + (1 - beta1) * self.dby\n",
        "        self.vby = beta2 * self.vby + (1 - beta2) * np.square(self.dby)\n",
        "        m_hat = self.mby / (1 - beta1)\n",
        "        v_hat = self.vby / (1 - beta2)\n",
        "        self.by -= self.learning_rate * (m_hat / (np.sqrt(v_hat) + epsilon) + L2_reg * self.by)\n",
        "\n",
        "    def train(self, data_generator,iterations):\n",
        "        \"\"\"\n",
        "        Train the GRU on a dataset using backpropagation through time.\n",
        "\n",
        "        Args:\n",
        "            data_generator: An instance of DataGenerator containing the training data.\n",
        "\n",
        "        Returns:\n",
        "            None\n",
        "        \"\"\"\n",
        "        iter_num = 0\n",
        "        # stopping criterion for training\n",
        "        threshold = 50\n",
        "\n",
        "        smooth_loss = -np.log(1.0 / data_generator.vocab_size) * self.sequence_length  # initialize loss\n",
        "        while (iter_num < iterations):\n",
        "            # initialize hidden state at the beginning of each sequence\n",
        "            if data_generator.pointer == 0:\n",
        "                c_prev = np.zeros((self.hidden_size, 1))\n",
        "                a_prev = np.zeros((self.hidden_size, 1))\n",
        "\n",
        "            # get a batch of inputs and targets\n",
        "            inputs, targets = data_generator.next_batch()\n",
        "\n",
        "            # forward pass\n",
        "            X, r, z, c, cc, a, y_pred = self.forward(inputs, c_prev, a_prev)\n",
        "\n",
        "            # backward pass\n",
        "            self.backward(X, a_prev, c_prev, r, z, c, cc, a, y_pred, targets)\n",
        "\n",
        "            # calculate and update loss\n",
        "            loss = self.loss(y_pred, targets)\n",
        "            self.adamw()\n",
        "            smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
        "            # update previous hidden state for the next batch\n",
        "            a_prev = a[self.sequence_length - 1]\n",
        "            c_prev = c[self.sequence_length - 1]\n",
        "            # if iter_num == 5900 or iter_num == 30000:\n",
        "            #             self.learning_rate *= 0.1\n",
        "            # print progress every 100 iterations\n",
        "            if iter_num % 1000 == 0:\n",
        "                self.learning_rate *= 0.9 # ativei o learning rate decay\n",
        "                sample_idx = self.sample(c_prev, a_prev, inputs[0, :], 200)\n",
        "                # print(sample_idx)\n",
        "                # print(idx for idx in sample_idx)\n",
        "                # print(data_generator.idx_to_char[int(idx)] for idx in sample_idx)\n",
        "                print(''.join(data_generator.idx_to_char[int(idx)] for idx in sample_idx))  # adaptação para cupy\n",
        "                print(\"\\n\\niter :%d, loss:%f\" % (iter_num, smooth_loss))\n",
        "            iter_num += 1\n",
        "\n",
        "    def sample(self, c_prev, a_prev, seed_idx, n):\n",
        "        \"\"\"\n",
        "        Sample a sequence of integers from the model.\n",
        "\n",
        "        Args:\n",
        "            c_prev (numpy.ndarray): Previous cell state, a numpy array of shape (hidden_size, 1).\n",
        "            a_prev (numpy.ndarray): Previous hidden state, a numpy array of shape (hidden_size, 1).\n",
        "            seed_idx (numpy.ndarray): Seed letter from the first time step, a numpy array of shape (vocab_size, 1).\n",
        "            n (int): Number of characters to generate.\n",
        "\n",
        "        Returns:\n",
        "            list: A list of integers representing the generated sequence.\n",
        "\n",
        "        \"\"\"\n",
        "        # initialize input and seed_idx\n",
        "        x = np.zeros((self.vocab_size, 1))\n",
        "        # convert one-hot encoding to integer index\n",
        "        seed_idx = np.argmax(seed_idx, axis=-1)\n",
        "\n",
        "        # set the seed letter as the input for the first time step\n",
        "        x[seed_idx] = 1\n",
        "\n",
        "        # generate sequence of characters\n",
        "        idxes = []\n",
        "        c = np.copy(c_prev)\n",
        "        a = np.copy(a_prev)\n",
        "        for t in range(n):\n",
        "            # compute the hidden state and cell state\n",
        "            concat = np.vstack((a, x))\n",
        "            z = self.sigmoid(np.dot(self.Wz, concat) + self.bz)\n",
        "            r = self.sigmoid(np.dot(self.Wr, concat) + self.br)\n",
        "            cc = np.tanh(np.dot(self.Wa, np.vstack((r * a, x))) + self.ba)\n",
        "            c = z * c + (1 - z) * cc\n",
        "            a = c\n",
        "            # compute the output probabilities\n",
        "            y = self.softmax(np.dot(self.Wy, a) + self.by)\n",
        "\n",
        "            # sample the next character from the output probabilities\n",
        "            idx = np.random.choice(range(self.vocab_size), size=1, p=y.ravel())\n",
        "\n",
        "            # set the input for the next time step\n",
        "            x = np.zeros((self.vocab_size, 1))\n",
        "            x[idx] = 1\n",
        "\n",
        "            # append the sampled character to the sequence\n",
        "            idxes.append(idx)\n",
        "\n",
        "        # return the generated sequence\n",
        "        return idxes\n",
        "\n",
        "    def predict(self, data_generator, start, n):\n",
        "        \"\"\"\n",
        "        Generate a sequence of n characters using the trained GRU model, starting from the given start sequence.\n",
        "\n",
        "        Args:\n",
        "        - data_generator: an instance of DataGenerator\n",
        "        - start: a string containing the start sequence\n",
        "        - n: an integer indicating the length of the generated sequence\n",
        "\n",
        "        Returns:\n",
        "        - txt: a string containing the generated sequence\n",
        "        \"\"\"\n",
        "        # initialize input sequence\n",
        "        x = np.zeros((self.vocab_size, 1))\n",
        "        chars = [ch for ch in start]\n",
        "        idxes = []\n",
        "        for i in range(len(chars)):\n",
        "            idx = data_generator.char_to_idx[chars[i]]\n",
        "            x[idx] = 1\n",
        "            idxes.append(idx)\n",
        "        # initialize cell state and hidden state\n",
        "        a = np.zeros((self.hidden_size, 1))\n",
        "        c = np.zeros((self.hidden_size, 1))\n",
        "\n",
        "        # generate new sequence of characters\n",
        "        for t in range(n):\n",
        "            # compute the hidden state and cell state\n",
        "            concat = np.vstack((a, x))\n",
        "\n",
        "            # compute the reset gate\n",
        "            r = self.sigmoid(np.dot(self.Wr, concat) + self.br)\n",
        "\n",
        "            # compute the update gate\n",
        "            z = self.sigmoid(np.dot(self.Wz, concat) + self.bz)\n",
        "\n",
        "            # compute the candidate cell state\n",
        "            cc = np.tanh(np.dot(self.Wa, np.vstack((r * a, x))) + self.ba)\n",
        "\n",
        "            # compute the cell state\n",
        "            c = z * cc + (1 - z) * c\n",
        "\n",
        "            # compute the hidden state\n",
        "            a = c\n",
        "\n",
        "            # compute the output probability vector\n",
        "            y_pred = self.softmax(np.dot(self.Wy, a) + self.by)\n",
        "            # sample the next character from the output probabilities\n",
        "            idx = np.random.choice(range(self.vocab_size), size=1, p=y_pred.ravel())\n",
        "            x = np.zeros((self.vocab_size, 1))\n",
        "            x[idx] = 1\n",
        "            idxes.append(int(idx)) # adaptação para cupy\n",
        "        txt = ''.join(data_generator.idx_to_char[int(i)] for i in idxes)\n",
        "        return txt\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-12T11:58:41.196289Z",
          "iopub.execute_input": "2023-03-12T11:58:41.196778Z",
          "iopub.status.idle": "2023-03-12T11:58:41.272325Z",
          "shell.execute_reply.started": "2023-03-12T11:58:41.196738Z",
          "shell.execute_reply": "2023-03-12T11:58:41.270963Z"
        },
        "trusted": true,
        "id": "svdmDlM9Ka4H"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequence_length = 24\n",
        "#read text from the \"input.txt\" file\n",
        "data_generator = DataGenerator('/kaggle/input/machado-de-assis/raw/txt/romance/memoriasBras.txt', sequence_length)\n",
        "gru =  GRU(hidden_size=1000, vocab_size=data_generator.vocab_size,sequence_length=sequence_length,learning_rate=0.005) # aumentei a quantidade de neuronios e learning rate\n",
        "\n",
        "gru.train(data_generator,iterations=15000)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-12T11:58:41.275022Z",
          "iopub.execute_input": "2023-03-12T11:58:41.276187Z",
          "iopub.status.idle": "2023-03-12T12:01:02.460464Z",
          "shell.execute_reply.started": "2023-03-12T11:58:41.276131Z",
          "shell.execute_reply": "2023-03-12T12:01:02.459205Z"
        },
        "trusted": true,
        "id": "Azvt0YZgKa4L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "837d6c21-c708-4058-b6e3-6699672b386b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-001e8cb65a04>:346: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  print(''.join(data_generator.idx_to_char[int(idx)] for idx in sample_idx))  # adaptação para cupy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "úÈÉcí7,-9i;à)C\n",
            "ÂaHdÊÉ. —FèÈUh Üp)èmÜ.8èvhÜ?zõuxFó?GÈñwÛhüZ5(wÁrT\n",
            "”kp0SOf:5wJ 1óuVN2r/êáIV7çNÀMúZKCV'OL9x”é5)OnçêcÍü'?AÃM“ú0âíÇA3VCu_oõê8)/poÉgÀpÃa)cBZiéÉl2PG71GvI3ráÁtgÇÃP\n",
            "1’hSJÔ8!Hxtr’_nÁM6xilP\n",
            "ID. Ã\n",
            "\n",
            "\n",
            "iter :0, loss:112.592297\n",
            " pentounha menfama inho uciam eputer abor agrgivalado se nhomure se pera os me um\n",
            "eplevo-e-má--selóer ous epao gido e ime se pore peranfiu-se o pepla vuimdaim anôgoganflaio couis\n",
            "nho maia\n",
            "igutantanise\n",
            "\n",
            "\n",
            "iter :1000, loss:85.117291\n",
            "eu, ndor dos do o alotíindedo dero, pondo só rotírâncatendo. E Não nhônado da rmão, do punta do piceinoça, deur do rioncios de unddis, e dio ddeiro dascênicons elu greincos, cbicêncado dide\n",
            "dede ossce\n",
            "\n",
            "\n",
            "iter :2000, loss:73.306393\n",
            "aera o da\n",
            "écomo melurdo do coi moidi-mesetra umo deu dima reusto, ar ates velaso epatrra. A Ercanãod peurvelo deze as devraos mima laze. NA\n",
            "Tele.. A E PLE\n",
            "rrcumel.... — Er úlmué celvir-ele do cera deu\n",
            "\n",
            "\n",
            "iter :3000, loss:68.588654\n",
            "entoque eto felaz, ques anetra negir?\n",
            "Vraze mer savial trho fecintanfosleito que a ficletro mentosa tre nsossabous co melhai. De cou e\n",
            "dialte:\n",
            "— Viraginte ma meranta me tea nsime.\n",
            "inhera felia cade nã\n",
            "\n",
            "\n",
            "iter :5000, loss:65.409730\n",
            "munté, cum mão\n",
            "dovito, das iíla, celova envea ma, antaré das uspeitar\n",
            "aregentaman\n",
            "asme nacem o\n",
            "citonqüuê-lhe cosa senha caste econado raístointro?\n",
            "CPorotoria sa, emistinhe da cespatina ra umistenangüê\n",
            "\n",
            "\n",
            "iter :6000, loss:64.204284\n",
            " tão conção go, de o o toste ve ita angunde tina-se jou\n",
            "ncome nação o te ede hsos vezre vatão pos ente, quenha udio m guma\n",
            "este baisas ote os posa do té com om gomostindizo mo ssâncincianta mavala....\n",
            "\n",
            "\n",
            "iter :7000, loss:63.044072\n",
            "iseis entro dia efiséi gumis, dipos? sincha caponsa de dos bara, os deise umo ro quia\n",
            "preuta do fiso? LVílijaa\n",
            "anto-m à sa andas; haidos, diar peligíalm................................................\n",
            "\n",
            "\n",
            "iter :8000, loss:62.912390\n",
            "m sor olarita ope que emacaos, enas merixa mera me acas, de tideventes olher-menlse desó crirmoisto, naco\n",
            "tia via\n",
            "de biade a nem tevar eposso que qua evinela uem melhadeprinde que eura coros iaguar io\n",
            "\n",
            "\n",
            "iter :9000, loss:62.187307\n",
            "fíida aprantana alaino do mu dido, que e setuitva,\n",
            "— Lo XXXVI / Acha pícada aixar lástiasa queine o de mão des egruçõess om urasontvel, de Viagre pado camanda nocmase dasse gnoum tuisstama sabla, cmão\n",
            "\n",
            "\n",
            "iter :10000, loss:61.595478\n",
            "o ogília de rerito heme sesus ome cogua ortos neu bo do ou ferte eruto; sfosílimo fecera\n",
            "caingía doi, e nde so\n",
            "angüitor aco quela arra\n",
            "diácrada! — Abnime atrim umão fus! prdecisada a dos tde somícilo \n",
            "\n",
            "\n",
            "iter :11000, loss:61.355342\n",
            "rges, de eumo; duis da cinãse. \n",
            "— d'epreporiave o outr,\n",
            "que as emporade, outros pode.  Corite que então stâncácia!\n",
            "Viraga; ta a uzinvesssda olotre; des argenção Que da scda.  Qurevoise que o Des\n",
            "palgi\n",
            "\n",
            "\n",
            "iter :12000, loss:61.597398\n",
            "ivara exumoa elitra dos queltorção, e cema\n",
            "paria do queu cadeçadco.\n",
            " Pale.\n",
            "E Plávia do plóraso\n",
            "Nero eme pronfo. O Huma às mprórgingüinde, depentemcendes\n",
            "des queno de plótida à ecride manugo; faco de u\n",
            "\n",
            "\n",
            "iter :13000, loss:62.992582\n",
            "s que ala cerenca de melume carimoude do não codê cimeinfor cama pela came tare, ponsa guiva ndo mensa dosse loris omuto ongroner milmentalvezi-me,\n",
            "dhino me gala palu dão meplinceda eixume, eixparatra\n",
            "\n",
            "\n",
            "iter :14000, loss:60.114579\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gru.predict(data_generator, \"empl\", 1000)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-12T12:01:22.082753Z",
          "iopub.execute_input": "2023-03-12T12:01:22.083181Z",
          "iopub.status.idle": "2023-03-12T12:01:22.448817Z",
          "shell.execute_reply.started": "2023-03-12T12:01:22.083146Z",
          "shell.execute_reply": "2023-03-12T12:01:22.447534Z"
        },
        "trusted": true,
        "id": "HRVDcp6_Ka4M",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "outputId": "a6c08119-fd38-4f78-ff05-475313b9287d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-001e8cb65a04>:450: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  idxes.append(int(idx)) # adaptação para cupy\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'empletes pelcobra, o cal menstrês lom que ere de se gada que etro derilos obé! Aque pescodê cinsupas Ou castâncimata; e Já clicotéims que au do has onósrina deitrima simu mem sisem deras.  Sanhão dis deide eitando que e láguma ampra dongulha mo dem ladicado oguando\\nprosó pronde e éu tarde abave-so e\\nde mrasida.\\n— QU BASbNTAREÇÃO\\nRIMOMa,\\nxcuntis ceigos apor dospaçao dafa anscantefa deisterta,\\náginse om sastão de qua cavimodo quare de sbanado cemathá-mwlem im quane ére ata cas gastar ode seco o se maranva qui lhergüindio?\\nVigruados anão não?\\n— Que no im diza falgrito de nical mecoficasinodir, vão decre, mema girom imo lte scrêncilas se\\ndavi-mo do\\npesto.\\nCum oso de cha ve a só Reguise dere de\\ndisimo-s ema decasstenje ade enga, a\\nmigum dairo; matrentara va cripare, e utrtam, antida portora pontos dis Vigarsecorto\\nãos huia ixtase ama isas Bér inolni diavistoblimo.  APlam nou vistro njálimparte, do ocoté se\\npararse fou carertou-lhemo no cor mecase dere de ma sa úbcintave-s e mestios que emiate, '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"4\"></a>\n",
        "<h1 style='background:#FF9F00;border:0; color:black;\n",
        "    box-shadow: 10px 10px 5px 0px rgba(0,0,0,0.75);\n",
        "    transform: rotateX(10deg);\n",
        "    '><center style='color: #3E3D53;'>Thank you</center></h1>\n",
        "\n",
        "# Thank you\n",
        "\n",
        "**Thank you for going through this notebook**\n",
        "\n",
        "**If you have any suggestions please let me know**\n",
        "\n",
        "<a id=\"5\"></a>\n",
        "# References\n",
        "https://gist.github.com/karpathy/d4dee566867f8291f086\n",
        "\n",
        "https://arxiv.org/abs/1711.05101"
      ],
      "metadata": {
        "id": "p6SWrz2sKa4N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"padding:10px;\n",
        "            color:#333333;\n",
        "            margin:10px;\n",
        "            font-size:150%;\n",
        "            display:fill;\n",
        "            border-radius:1px;\n",
        "            border-style:solid;\n",
        "            border-color:#666666;\n",
        "            background-color:#F9F9F9;\n",
        "            overflow:hidden;\">\n",
        "    <center>\n",
        "        <a id='top'></a>\n",
        "        <b>Machine Learning From Scratch Series</b>\n",
        "    </center>\n",
        "    <br>\n",
        "    <ul>\n",
        "        <li>\n",
        "            <a href=\"https://www.kaggle.com/code/fareselmenshawii/linear-regression-from-scratch\" style=\"color:#0072B2\">1 - Linear Regression</a>\n",
        "        </li>\n",
        "        <li>\n",
        "            <a href=\"https://www.kaggle.com/code/fareselmenshawii/logistic-regression-from-scratch\" style=\"color:#0072B2\">2 -  Logistic Regression</a>\n",
        "        </li>\n",
        "        <li>\n",
        "            <a href=\"https://www.kaggle.com/code/fareselmenshawii/kmeans-from-scratch\" style=\"color:#0072B2\">3 - KMeans</a>\n",
        "        </li>\n",
        "        <li>\n",
        "            <a href=\"https://www.kaggle.com/code/fareselmenshawii/decision-tree-classifier-from-scratch\" style=\"color:#0072B2\">4 - Decision Trees</a>\n",
        "        </li>\n",
        "        <li>\n",
        "            <a href=\"https://www.kaggle.com/code/fareselmenshawii/random-forest-classifier-from-scratch\" style=\"color:#0072B2\">5 -  Random Forest</a>\n",
        "        </li>\n",
        "        <li>\n",
        "            <a href=\"https://www.kaggle.com/code/fareselmenshawii/knn-from-scratch\" style=\"color:#0072B2\">6 - KNearestNeighbor</a>\n",
        "        </li>\n",
        "        <li>\n",
        "            <a href=\"https://www.kaggle.com/code/fareselmenshawii/pca-from-scratch?scriptVersionId=121402593\" style=\"color:#0072B2\">7 - PCA</a>\n",
        "        </li>\n",
        "        <li>\n",
        "            <a href=\"https://www.kaggle.com/code/fareselmenshawii/svm-from-scratch\" style=\"color:#0072B2\">8 - SVM</a>\n",
        "        </li>\n",
        "        <li>\n",
        "            <a href=\"https://www.kaggle.com/code/fareselmenshawii/naive-bayes-from-scratch\" style=\"color:#0072B2\">9 - Naive Baye</a>\n",
        "        </li>\n",
        "        <li>\n",
        "            <a href=\"https://www.kaggle.com/code/fareselmenshawii/optimized-neural-network-from-scratch\" style=\"color:#0072B2\">10 - Optimized Neural Network</a>\n",
        "        </li>\n",
        "        <li>\n",
        "            <a href=\"https://www.kaggle.com/code/fareselmenshawii/neural-network-from-scratch\" style=\"color:#0072B2\">11 - Neural Network</a>\n",
        "        </li>\n",
        "        <li>\n",
        "            <a href=\"https://www.kaggle.com/code/fareselmenshawii/cnn-from-scratch\" style=\"color:#0072B2\">12 - CNN</a>\n",
        "        </li>\n",
        "        <li>\n",
        "            <a href=\"https://www.kaggle.com/code/fareselmenshawii/rnn-from-scratch\" style=\"color:#0072B2\">13 - RNN</a>\n",
        "        </li>\n",
        "        <li>\n",
        "            <a href=\"https://www.kaggle.com/code/fareselmenshawii/lstm-from-scratch\" style=\"color:#0072B2\">14 - LSTM</a>\n",
        "        </li>\n",
        "        <li>\n",
        "            <a href=\"https://www.kaggle.com/code/fareselmenshawii/gru-from-scratch\" style=\"color:#0072B2\">15 - GRU</a>\n",
        "        </li>\n",
        "    </ul>\n",
        "</div>"
      ],
      "metadata": {
        "id": "gytfC2DzKa4N"
      }
    }
  ]
}